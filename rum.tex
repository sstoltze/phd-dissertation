


% Skal der laves et intro-kapitel med baggrundsmateriale? Løkker
% (Milnor), Bruhat, spektralfølger.
% Ellers skal det nok ind her et sted, men måske spredt lidt ud over
% kapitlet.

\chapter{Spaces of linearly independent vectors}
\label{chap:rum}

% Introduktion til rum
%% Ukvotienterede (Y_{m,n}) og de rigtige (X_{m,n} = Y_{m,n}/T^m)
%%% Så introducer gruppevirkningen her. Og vis at Y \cong X x T
%%% Bør nok også nævne virkningen på rækkerne, da den spiller ind i
%%% spektralfølgen.
%% Andre permutationer.
%% Andre koefficienter: \R og \H. Disse dukker først op igen
%% senere, men det meste af teorien i dette afsnit burde være
%% uafhængigt af koefficienterne, op til at nogle skalarer flytter
%% lidt rundt. Specielt kan vi lige så godt smide dem ind her.

In this chapter we will introduce the spaces that will be studied in
the rest of the document, and collect various useful
results.

\section{Spaces} 
\label{sec:rum}
Before we can define the spaces that we will be working with, it will
be helpful to define an approximation to them.

\begin{definition} 
  For natural numbers $m$ and $n$, the space $Y_{m,n}\subset
  (\C^{m})^{n}$ is
  \[ Y_{m,n} = \set{(a_1,\dots,a_n) \in (\C^m)^n \delim\Bigg\vert\delim
    \begin{matrix}
      \text{Any } m \text{ subsequent vectors in } \\
      (e_1,\dots,e_m,a_1,\dots,a_n,e_1,\dots,e_m) \\
      \text{ are linearly independent.}
    \end{matrix} }, \]
  where $e_1,\dots,e_m \in \C^m$ denotes the standard basis
  vectors.
\end{definition}

We will switch between considering $(a_1,\dots,a_n)\in Y_{m,n}$ as an
ordered sequences of linearly independent vectors and an $m\times n$
matrix $[a_1,\dots,a_n]$ as needed and without mention. We can replace
the standard basis that appears in the definition of $Y_{m,n}$ with
any collection of linearly independent vectors and get similarly
defined spaces: 
\begin{definition}
  \label{def:rum}
  For two invertible matrices $X$ and $Y$ in
  $\GL_m(\C)$, the space \newline
  $Y_{m,n}(X,Y) \subset (\C^{m})^{n}$ is
  \[ Y_{m,n}(X,Y) = \set{(a_1,\dots,a_n) \in (\C^m)^n
    \delim\Bigg\vert\delim 
    \begin{matrix}
      \text{Any } m \text{ subsequent vectors in } \\
      (x_1,\dots,x_m,a_1,\dots,a_n,y_1,\dots,y_m) \\
      \text{ are linearly independent.}
    \end{matrix} }, \]
  where $(x_1,\dots,x_m)$ and $(y_1,\dots,y_m)$ are the column vectors
  of $X$ and $Y$ respectively.
  
  There are two special cases that will be important.
  If both $X$ and $Y$ are the identity-matrix, we get the previously
  defined spaces,
  \[ Y_{m,n} = Y_{m,n}(\Id,\Id). \]
  When $X$ is the identity-matrix, we will usually omit it from the
  notation and instead write
  \[ Y_{m,n}(Y) = Y_{m,n}(\Id,Y). \]
\end{definition}

We could also define $Y_{m,n}(X,Y)$ as the
pre-image of the open set $(\C^*)^{m+n-1}$ in $\C^{m+n-1}$ under the
continuous map $\Det_{X,Y}$ that calculates determinants of subsequent
vectors:
\begin{align*}
  \Det_{X,Y} : (\C^m)^n \to &\;\C^{m+n-1} \\
  \Det_{X,Y}(a_1,\dots,a_n) = \,&\; (\det(x_2,\dots,x_m,a_1),\\
  &\; \det(x_3,\dots,x_m,a_1,a_2), \\
  &\;\;\;\; \vdots\\
  &\; \det(a_n,y_1,\dots,y_{m-1})).
\end{align*}
This shows that $Y_{m,n}(X,Y)$ is an open subset of
$(\C^m)^n$. Another important
feature of $Y_{m,n}(X,Y)$ is that it does not really depend on the
particular matrices
$X =\nobreak [x_1,\dots,x_m]$ and $Y = [y_1,\dots,y_m]$,
but only on the two flags in $\C^{m}$ defined from the columns:
\begin{alignat*}{5}
  \mathrm{Fl_R}(X)\, &&= \Big(&\spa(x_m) &&\subset \spa(x_{m-1},x_m)
  &&\subset \dots &&\subset \C^m\Big), \\
  \mathrm{Fl_L}(Y)\, &&= \Big(&\spa(y_1) &&\subset \spa(y_1,y_2)
  &&\subset \dots &&\subset \C^m\Big).
\end{alignat*}
This is because we are considering linear independence of the vectors
\[ (x_1,\dots,x_m,a_1,\dots,a_n,y_1,\dots,y_n), \]
and if $(a_1,\dots,a_{k})$ is a collection of linearly independent
vectors of $\C^m$, then the vectors
$(x_{k+1},\dots,x_m,a_1,\dots,a_{k})$ are
linearly independent if and only if $a_1$ is not in the subspace
spanned by $(x_{k+1},\dots,x_m)$, $a_1$ and $a_2$ are not in the
subspace spanned by $(x_{k+2},\dots,x_m)$ and so forth. The same
argument can be applied to the columns of $Y$.

At first glance it looks like we are considering a huge collection of
spaces, but most of them are indistinguishable topologically, as the
following lemma shows.

\begin{lemma} \label{lem:rum-perm}
  For $X,Y\in \GL_m(\C)$, there exists a unique permutation $\sigma
  \in S_m$ such that the space
  $Y_{m,n}(X,Y)$ is homeomorphic to $Y_{m,n}(\sigma)$.
\end{lemma}

\begin{proof}
  Since $X$ is invertible it preserves linear independence of vectors,
  and we get a homeomorphism
  \begin{align*}
    Y_{m,n}(X,Y) &\to Y_{m,n}(\Id,X^{-1}Y) \\
    (a_1,\dots,a_m) &\mapsto (X^{-1}a_1,\dots,X^{-1}a_m).
  \end{align*}
  The matrix $X^{-1}Y$ is invertible, and by using the
  Bruhat-decomposition of $\GL_m(\C)$, as in Section \ref{sec:bruhat},
  we can write it uniquely as a product 
  \[ X^{-1}Y = L\sigma U, \]
  where $L$ is lower-triangular, $U$ is upper-triangular and $\sigma$
  is a permutation. Hence we get a homeomorphism,
  \[ Y_{m,n}(X,Y) \cong Y_{m,n}(L^{-1}, \sigma U). \]
  But by the remark above, the space $Y_{m,n}(L^{-1},\sigma U)$ only
  depends
  on the flags 
  \begin{align*}
    \mathrm{Fl_R}(L^{-1}) &= \mathrm{Fl_R}(\Id), \\
    \mathrm{Fl_L}(\sigma U) &= \mathrm{Fl_L}(\sigma).
  \end{align*}
  So $Y_{m,n}(L^{-1},\sigma U)$ is the same as the space
  $Y_{m,n}(\sigma)$.
\end{proof}

Due to this, we will almost exclusively consider the spaces
$Y_{m,n}(\sigma)$ throughout the text. To get a feel for the spaces,
we will now work with some simple examples and compute their
cohomology.

\begin{example}
  \label{ex:n=1}

  The space $Y_{m,1}$ is homeomorphic to $(\C^*)^m$, since
  \[\begin{pmatrix}
    1&0&\dots&0&\l_1&1&0&\dots&0 \\
    0&1&\dots&0&\l_2&0&1&\dots&0 \\
    \vdots&\vdots&\ddots&\vdots&\vdots&\vdots&\vdots&\ddots&\vdots \\
    0&0&\dots&1&\l_m&0&0&\dots&1
  \end{pmatrix} \]
  has linearly independent columns if and only if all the scalars
  $\l_1,\dots,\l_m$ are invertible. The cohomology is
  \[ H^q(Y_{m,1}) \cong H^q((\C^*)^m) = \Z^{{m \choose q}}. \]
\end{example}

\begin{example}
  \label{ex:22}

  The space $Y_{2,2}$ is
  \[ \set{
    \begin{pmatrix}
      a & b \\
      c & d
    \end{pmatrix} \delim\bigg\vert\delim a \neq 0, d \neq 0, ad-bc
    \neq 0 } \cong (\C^*)^2 \times \set{(b,c)\in\C^2 \delim\vert\delim
    bc \neq 1}, \]
  with the homeomorphism given by
  \[
  \begin{pmatrix}
    a & b\\
    c & d
  \end{pmatrix} \mapsto \left(a,d,\frac{b}{d},\frac{c}{a}\right).\]
  The product of the two fractions is
  \[ \frac{bc}{ad} = \frac{bc-ad}{ad} + 1 = 1-\frac{ad-bc}{ad} \neq
  1, \]
  since $ad-bc \neq 0$.

  The space $\set{(b,c) \in \C^2 \delim\vert\delim bc\neq 1}$ will be
  denoted $Y$. We would like to compute the cohomology of this space,
  since we can then use the K\"unneth formula to find the cohomology
  of $Y_{2,2}$.
  Consider $Y$ as a subspace of
  the one-point compactification $S^4 = \C^2\cup\set{\infty}$. Then the
  complement is
  \[ S^4-Y = \set{(b,c)\in \C^2 \mid bc=1}\cup\set{\infty}. \]
  This can be identified with the wedge sum $S^1\vee S^2$ by considering
  the subspace
  \[ \C^* \cong \set{\left(b,\frac{1}{b} \right) \sdel b\in \C^*} =
  (S^4-Y)-\set{\infty}. \]
  The point at infinity is glued to this space when either $b$ or $c$
  is large, which for our copy of $\C^*$
  corresponds to $b$ large or $b$ near 0. Hence what we get is
  $S^2=\C\cup\set{\infty}$ with the identification $0 \sim \infty$. This
  is homotopy equivalent to $S^1\vee S^2$.
  
  Now we can apply Alexander duality to this space to compute
  \[ \widetilde H_q(Y) \cong \widetilde H^{3-q}(S^4-Y) =
  \begin{cases}
    \Z & q \in \set{1,2}, \\
    0 & \text{otherwise}.
  \end{cases} \]
  and hence
  \[ H^q(Y) \cong
  \begin{cases}
    \Z & q \in \set{0,1,2}, \\
    0 & \text{otherwise}.
  \end{cases} \]
  
  This shows
  \[ H^q(Y_{2,2}) \cong H^q((\C^*)^2\times Y) =
  \begin{cases}
    \Z & q=0, \\
    \Z^3 & q = 1, \\
    \Z^4 & q = 2, \\
    \Z^3 & q = 3, \\
    \Z & q =4, \\
    0 & \text{otherwise}.
  \end{cases} \]
\end{example}

\begin{example}
  \label{ex:32}

  The space $Y_{3,2}$ is
  \[\set{
    \begin{pmatrix}
      a &b \\
      c & d \\
      e & f
    \end{pmatrix} \delim\Bigg\vert\delim a \neq 0, f \neq 0, ad-bc \neq
    0, cf-ed \neq 0}.
  \]
  This is homeomorphic to $(\C^*)^4\times \set{(x,y,z)\in\C^3 \mid
    y-z-xyz = 1}$, with the map given by
  \[
  \begin{pmatrix}
    a &b \\
    c & d \\
    e & f
  \end{pmatrix} \mapsto
  \left(a, f, D=\frac{ad-bc}{af}, C=\frac{cf-de}{af}, x=\frac{bC}{fD},
    y=\frac{c}{aC}, z=\frac{eD}{aC} \right). \]
  Some of the coordinates have been named to shorten the writing
  slightly. By direct calculation, the last three coordinates satisfy
  the relation
  \begin{align*}
    y-z-xyz &= \frac{c}{aC}-\frac{eD}{aC}-\frac{bce}{a^2fC} \\
            &= \frac{acf-aefD-bce}{a^2fC} \\
            &= \frac{acf-e(ad-bc)-bce}{a(cf-de)}  \\
            &= \frac{acf-ade}{acf-ade} \\
            &= 1.
  \end{align*}
  The inverse map is
  \[ (a,f,D,C,x,y,z) \mapsto
  \begin{pmatrix}
    a & \frac{fDx}{C} \\
    aCy & fD(1+xy) \\
    \frac{aCz}{D} & f
  \end{pmatrix}. \]

  We would like to compute the cohomology of the space 
  \[ Z = \set{ (x,y,z)\in\C^3 \mid y-z -xyz = 1}. \]
  This is the same as the cohomology of $S^2$, but this is hard to see
  directly. Instead consider the space
  \[ \widetilde Z = \set{(b,c,d,e) \in \C^4 \mid d\neq
    bc, c\neq de}. \]
  This space is homeomorphic to $Z\times (\C^*)^2$:
  \begin{align*}
    (b,c,d,e) \mapsto \left(
    \frac{b(c-de)}{d-bc}, \frac{c}{c-de}, \frac{e(d-bc)}{c-de},
    d-bc, c-de\right).
  \end{align*}
  We will use Alexander duality to compute the cohomology. Consider
  the complement in $S^8$,
  \[ S^8-\widetilde Z = \set{(b,c,d,e) \mid d = bc} \cup \set{(b,c,d,e)
    \mid c = de} \cup \set{\infty}, \]
  and denote the two sets as $W_1$ and $W_2$. Then
  \begin{align*}
    W_1\cap W_2 &= \set{(b,c,d,e) \mid d=bc,c=de}\cup\set{\infty}.
  \end{align*}
  We glue the point at infinity to this space when $c$ is large, but
  this is the same as either $d$ or $e$ being large. Hence there is a
  homeomorphism,
  \begin{align*}
    W_1\cap W_2 &\cong \set{(b,d,e) \mid d=bde}\cup\set{\infty} \\
                &= \set{(b,d,e)\mid 0 = (be-1)d}\cup\set{\infty} \\
                &= \set{(b,d,e) \mid d = 0} \cup \set{(b,d,e) \mid
                  be=1}\cup\set{\infty}.
  \end{align*}
  
  These new spaces have computable homology, since they can be
  identified with spaces built from spheres. The first is relatively
  easy,
  \[ Z_1 = \set{(b,d,e)\mid d = 0}\cup \set{\infty} =
  \set{(b,0,0,d)}\cup\set{\infty} \cong S^4. \]
  The second is slightly harder.
  \[ Z_2 =\set{(b,d,e)\mid be=1}\cup\set{\infty}=
  \set{\left(b,\frac{d}{b},d,\frac{1}{b} \right) }\cup\set{\infty}. \]
  This set is homeomorphic to the one-point compactification of
  $\set{(b,d)\in \C^*\times \C}$. By \cite[Chapter~2.3]{dupontk}, this
  is given by the smash product of the one-point compactifications,
  \[ Z_2\simeq (\C^*\times\C)^+ \cong (\C^*)^+\wedge\C^+ \cong (S^1\vee
  S^2) \wedge S^2 = S^3 \vee S^4. \]
  The intersection of these spaces is
  \[ Z_1\cap Z_2 = \set{\left(b,0,0,\frac{1}{b}\right)}\cup\set{\infty}
  \simeq S^1\vee S^2. \]
  
  From all of this, we get a commutative diagram of inclusions,
  \[ \xymatrix{
    S^1\vee S^2 \ar[r]\ar[d] & S^3\vee S^4 \ar[d] \\
    S^4 \ar[r] & W_1\cap W_2.
  } \]
  The two maps from $S^1\vee S^2$ are zero in cohomology (when the
  degree is greater than zero) since in each
  degree either the domain or the codomain is zero. By applying
  Mayer-Vietoris, we get
  \[ H^q(W_1\cap W_2) =
  \begin{cases}
    \Z & q=0, \\
    0 & q = 1, \\
    \Z & q = 2, \\
    \Z^2 & q = 3, \\
    \Z^2 & q =4, \\
    0 & \text{otherwise}.
  \end{cases} \]
  where the case $q=0$ follows from $W_1\cap W_2$ being connected
  and $q = 1$ follows from writing out the relevant part of the
  long exact sequence,
  \[ \xymatrix{ H^0(S^3\vee S^4)\oplus H^0(S^4) \ar@{->>}[r] &
    H^0(S^1\vee S^4) \ar[r] & H^1(W_1\cap W_2) \ar[r] & 0.} \]
  
  $W_1$ and $W_2$ are both a copy of $S^6$ since we can get one of
  the coordinates from two of the others. Hence we get another diagram,
  \[ \xymatrix{
    W_1\cap W_2 \ar[r]\ar[d] & S^6 \ar[d] \\
    S^6 \ar[r] & S^8-\widetilde Z,
  } \]
  and again the map from the intersection is zero. Repeating the
  above, we can compute:
  \[ H^q(S^8-\widetilde Z) =
  \begin{cases}
    \Z & q = 0, \\
    0 & q = 1, \\
    0 & q = 2, \\
    \Z & q = 3, \\
    \Z^2 & q = 4, \\
    \Z^2 & q = 5, \\
    \Z^2 & q =6, \\
    0 & \text{otherwise}.
  \end{cases} \]
  Applying Alexander duality then gives
  \[ H^q(\widetilde Z) =
  \begin{cases}
    \Z & q = 0, \\
    \Z^2 & q = 1, \\
    \Z^2 & q = 2, \\
    \Z^2 & q = 3, \\
    \Z & q = 4, \\
    0 & \text{otherwise}.
  \end{cases} \]
  The shows that the cohomology of $Z$ is the same as the
  cohomology of $S^2$, if we factor out the torus. A more
  thorough description of the cohomology of $Z$ will be given in
  Section \ref{sec:ss33}.

  By using this, we can compute the cohomology of $Y_{3,2}$ as
  \[ H^q(Y_{3,2}) \cong H^q((\C^*)^4\times Z) = 
  \begin{cases}
    \Z & q = 0, \\
    \Z^4 & q = 1, \\
    \Z^7 & q = 2, \\
    \Z^8 & q = 3, \\
    \Z^7 & q = 4, \\
    \Z^4 & q = 5, \\
    \Z & q = 6, \\
    0 & \text{otherwise}.
  \end{cases} \]
\end{example}


% Vis at de er ikke-tomme
%% Nok både dimensions-argumentet og vise at stabiliseringen af e_1 er
%% i alle rummene.

To work with the above spaces, it will help to know that they are not
empty. For example, by considering the permutation $\big(1\; 3\big)$,
we see that it is impossible to satisfy the 
conditions for being in $Y_{3,1}\left(\big(1\; 3\big)\right)$. To see
this, take any vector $a_1$ in $\C^3$ and look at
$(e_1,e_2,e_3,a_1,e_3,e_2,e_1)$. If this is to be in the space, it
must have any three subsequent vectors linearly independent, but
$(e_3,a_1,e_3)$ is not invertible no matter what $a_1$ is. This shows
that the choice of $X$ and $Y$ in $Y_{m,n}(X,Y)$ plays a role in
determining when the spaces are empty. But as we shall now see, the
spaces are non-empty as long as we are not in an obviously empty
space. The exact meaning of ``obviously'' is given in the following
lemma.

\begin{theorem}
  \label{thm:ikke-tom}
  The space $Y_{m,n}(\sigma)$ is empty if and only if there exists an
  $i \in \set{1,\dots,m}$ such that
  \[ 1 + n + i \leq \sigma(i). \]
\end{theorem}
\begin{proof}
  The exact condition is slightly strangely worded, but if there is
  such an $i$, then the first and last columns of the matrix
  \[ (e_{\sigma(i)},\dots,e_m,a_1,\dots,a_n,\sigma_1,\dots,\sigma_i) \]
  are linearly dependent, since
  \[ \sigma_i = e_{\sigma(i)} \]
  by definition. By counting the three different groups of vectors, we
  see that the matrix has
  \[ m - \sigma(i) + 1 + n + i \]
  columns. The condition on $i$ shows that
  \[ m - \sigma(i) + 1 + n + i \leq m - \sigma(i) + \sigma(i) = m, \]
  so the conditions for being in $Y_{m,n}(\sigma)$ can never be
  satisfied, no matter the choice of vectors $a_1,\dots,a_n$. This
  shows one direction.

  The other direction
  will proceed by a purely dimensional argument. Consider the two
  flags of interest,
  \begin{alignat*}{6}
    \mathrm{Fl_R}(\Id) &&\;= (&V_1 &&\subset V_2 &&\subset \dots
    &&\subset V_m&&), \\
    \mathrm{Fl_L}(\sigma) &&\;= (&W_1 &&\subset W_2 &&\subset \dots
    &&\subset  W_m&&).
  \end{alignat*}

  Assume that the condition on $\sigma$ and $n$ is satisfied, so for
  all $i \leq m$:
  \[ 1 + n + i > \sigma(i). \]
  In the flag formulation, this means that the subspaces $V_{m-n-k}$
  and $W_k$ intersect trivially for all $k$.

  We must now find the vectors $a_1,\dots,a_n$ and will do so from
  left to right. By looking at the matrix
  \[ \left( e_1, \dots, e_m, a_1, \dots, a_n, \sigma_1, \dots,
    \sigma_m \right), \]
  we see that we must choose $a_1$ so it is not in any of the
  following subspaces of $\C^m$:
  \begin{align*}
    a_1 &\not\in V_{m-1}, \\
    a_1 &\not\in V_{m-n-1} \oplus W_1, \\
    \vdots & \\
    a_1 &\not\in V_1\oplus W_{m-n-1}, \\
    a_1 &\not\in W_{m-n}.
  \end{align*}
  But that means choosing $a_1$ in $\C^m$ after removing a
  finite number of proper subspaces, since the dimensions of the
  spaces on the right are $m-1$ for $V_{m-1}$ and $m-n$ for
  all the rest. This space is non-empty since $n\geq 1$, so this can
  always be done. We can continue this process inductively and get an
  element $(a_1,\dots,a_n)$ of $Y_{m,n}(\sigma)$, showing that the
  space is non-empty and concluding the proof.
\end{proof}
\begin{remark}
  Note that if $n \geq m-1$, then for any $i\geq 1$ we have
  \[ 1 + n + i \geq m - 1 + 1 +i \geq m+i > m \geq \sigma(i), \] 
  and hence the space $Y_{m,n}(\sigma)$ is non-empty. So the spaces
  become non-empty as
  soon as there are ``enough'' columns.
\end{remark}
It will occasionally be nice to have an explicit element of
$Y_{m,n}(\sigma)$, but to do this we will need a slight amount of
setup. Hence it will be delayed until Theorem
\ref{thm:eksplicit-element} in Section \ref{sec:rum-stabil}.

\section{Symmetries}

The space $Y_{m,n}(\sigma)$ has quite a few symmetries that could be
of interest. We will start with a particular one that will not be used
much in the dissertation, but which can occasionally be used to ease
computations.
\begin{lemma}
  \label{lem:transponering}
  If $(a_1,\dots,a_n) \in Y_{m,n}$, then the transposed matrix
  $(a_1,\dots,a_n)^T$ is in $Y_{n,m}$.
\end{lemma}
\begin{proof}
  This follows directly from defining $Y_{m,n}$ as
  $\Det_{\Id,\Id}^{-1}(\T^{m+n-1})$. When we compute the determinants
  of the matrix, we see that the important entries of
  $(a_1,\dots,a_n)$ lie in a square submatrix which is unchanged under
  transposition. So we get
  \begin{align*}
    \Det_{\Id,\Id}(a_1,\dots,a_n) &= (d_1,\dots,d_{m+n-1}) \\
    &= \Det_{\Id,\Id}( (a_1,\dots,a_n)^T ).
  \end{align*}
\end{proof}

As an example, we computed the cohomology of $Y_{3,2}$ in Example
\ref{ex:32}, and this theorem then tells us that
\[ Y_{2,3} \cong Y_{3,2} \cong (\C^*)^4\times Z \]
has the same cohomology groups.

Since we are interested in linear independence of vectors in $\C^m$,
the group of complex units, $\T=(\C)^*$, acts on the space 
in several ways. The 
most important for our purposes is the following:
\begin{definition}
  For any $i \in \set{1,\dots,n}$, the group $\T$ acts
  on $Y_{m,n}(\sigma)$ by scaling the $i$'th column vector,
  using the formula
  \[ \l \cdot (a_1,\dots,a_{i-1},a_i,a_{i+1},\dots,a_n) =
  (a_1,\dots,a_{i-1},\l a_i,a_{i+1},\dots,a_n). \]
  Since the determinant of a matrix is linear in each column, the
  linear independence of the vectors is preserved.

  All of these actions commute with each other, so we get an action of
  $\T^n$ on $Y_{m,n}(\sigma)$:
  \[ (\l_1,\dots,\l_n) \cdot (a_1,\dots,a_n) = (\l_1 a_1,\dots, \l_n
  a_n). \]
\end{definition}

While all of these symmetries are nice
to work with, the spaces as defined above have some issues. In
particular, as we shall see in Theorem \ref{thm:forste}, the
fundamental group and the cohomology ring of
$Y_{m,n}(\sigma)$ both become larger as $n$ grows. This complicates
some of our arguments, but luckily it can be helped by taking the
quotient with the above group action. This gives us the spaces we are
actually interested in.

\begin{definition}
  \label{def:kvotientrum}
  For natural numbers $n$ and $m$ and invertible matrices $X$ and $Y$,
  we define the space $X_{m,n}(X,Y)$ as the quotient of $Y_{m,n}(X,Y)$
  using the group action above,
  \[ X_{m,n}(X,Y) = Y_{m,n}(X,Y) / \T^n. \]
\end{definition}

Note that since matrix multiplication is linear, it commutes with the
action of $\T^n$ on $Y_{m,n}(X,Y)$ and we get directly from Lemma
\ref{lem:rum-perm} that for each choice of ${X,Y\in \GL_m(\C)}$ there
is a permutation $\sigma \in S_m$ such that
\[ X_{m,n}(X,Y) \cong X_{m,n}(\sigma). \]
Hence most of our attention will be focused on these particular
spaces. The quotient spaces are naturally closely related to the
spaces $Y_{m,n}(X,Y)$. In particular, we have the following:

\begin{lemma}
  \label{lem:reduktion}
  The spaces $Y_{m,n}(X,Y)$ and $X_{m,n}(X,Y) \times \T^n$ are
  homeomorphic.
\end{lemma}
\begin{proof}
  Define a map
  \[ X_{m,n}(X,Y) \to Y_{m,n}(X,Y) \]
  by mapping $A\cdot \T^n$ to the unique representative
  $\widetilde{A} \in Y_{m,n}(X,Y)$ with the last
  $n$ determinants all equal to one,
  \[ \Det_{X,Y}(\widetilde{A}) \in (\C^*)^{m-1} \times
  \{1\}^n. \]
  This map is continuous, as the composition with the quotient map,
  \[ \xymatrix{Y_{m,n}(X,Y) \ar[r]^q & X_{m,n}(X,Y) \ar[r] &
    Y_{m,n}(X,Y)}, \]
  is the map that uses the group action of $\T^n$ to change the last
  $n$ coordinates of $\Det_{X,Y}(A)$ to 1 and this is a continuous
  map. More precisely, it is given by the formula
  \[ (a_1,\dots,a_n) \mapsto \left(a_1,\dots,a_{m-1},\frac{1}{d_m}a_m,
    \frac{d_m}{d_{m+1}} a_{m+1},\dots,\hat{d}a_n\right), \]
  where $\Det_{X,Y}(a_1,\dots,a_n) = (d_1,\dots,d_{m+n-1})$ and the
  number $\hat{d}$ is an expression consisting of products and
  quotients of the last $n$ of these
  determinants, the exact terms of which depends on $n$ and $m$. If
  $\Det_{X,Y}(A) = (d_1,\dots,d_{m+n-1})$ denotes the determinants as
  above, the homeomorphism is the map
  \begin{align*}
    Y_{m,n}(X,Y) &\to X_{m,n}(X,Y) \times \T^n\\
    A &\mapsto (A\cdot \T^n, (d_m,\dots,d_{m+n-1})),
  \end{align*}
  with inverse
  \[ (A\cdot \T^n, (d_m,\dots, d_{m+n-1})) \mapsto
  \left(\overbrace{1,\dots,1}^{m-1},d_m,\frac{d_{m+1}}{d_m},\dots,
    \frac{1}{\hat{d}}\right) \cdot f(A). \]
\end{proof}

The above lemma is useful, since it oftens allows us to work with the
unquotiented spaces and still gain information about the
quotients.
This lemma also immediately proves the claim that the fundamental
group and cohomology ring of $Y_{m,n}(X,Y)$ grows with $n$, by the
formulas
\begin{align*}
  \pi_1(Y_{m,n}(X,Y)) &\cong \pi_1(X_{m,n}(X,Y)) \times \Z^n, \\
  H^*(Y_{m,n}(X,Y)) &\cong H^*(X_{m,n}(X,Y)) \otimes H^*(\T^n).
\end{align*}
But more importantly, the last formula allows us to calculate the
cohomology of $X_{m,n}(X,Y)$ from the cohomology of $Y_{m,n}(X,Y)$, by
quotienting out the free groups coming from the cohomology ring of the
torus $H^*(\T^n)$. For example, we will later compute 
\[ H^1(X_{m,n}) \cong \Z^{m-1} \]
by instead computing
\[ H^1(X_{m,n}) \oplus \Z^n \cong H^1(Y_{m,n}) \cong \Z^{m+n-1}. \]

% Stabilisering
%% Flere retninger? Vi bruger ikke rigtig X_{m,n} -> X_{m+1,n}
%% Dette kunne måske linkes til Bruhat (så en introduktion til dette?)
%% Og definer grænsen, X_{m,\infty}
\section{Stabilisation} 
\label{sec:rum-stabil}

We would like to relate the spaces $Y_{m,n}(\sigma)$ that we obtain
for different values of
$m$ and $n$. Since an invertible, lower-triangular matix $L$ preserves
the flag $\mathrm{Fl}_R(\Id)$, we will try to use this to define a map
\begin{align*}
  s : Y_{m,n} &\to Y_{m,n+1} \\
  s(a_1,\dots,a_{n}) &= (La_1,\dots,La_{n},Le_1).
\end{align*}
At first, consider the spaces $Y_{m,n}$ and $Y_{m,n+1}$.
To ensure the right hand side actually is an element of $Y_{m,n+1}$,
we will start by considering the inverse of $s$, mapping a subset of
$Y_{m,n+1}$ to $Y_{m,n}$.
Consider an element $A=(a_1,\dots,a_{n+1})$ in $Y_{m,n+1}$ where
the entries of $a_{n+1}$ are all equal to one:
\[ A = 
\begin{pmatrix}
  \vrule & \vrule & & \vrule & \vspace{-0.5em}
  1 \\
  a_1 & a_2 & \dots & a_{n} & \vdots \\
  \vrule & \vrule & & \vrule & 1
\end{pmatrix}. \] 
Then we can multiply through with the lower-triangular, invertible
matrix $L^{-1}$ with ones on the diagonal, negative ones just below
the diagonal and zeroes everywhere else,
\[ L^{-1} =
\begin{pmatrix}
   1 &  0 & 0 & \dots &  0 & 0 \\
  -1 &  1 & 0 & \dots &  0 & 0 \\
   0 & -1 & 1 & \dots &  0 & 0 \\
   \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
   0 &  0 & 0 & \dots &  1 & 0 \\
   0 &  0 & 0 & \dots & -1 & 1
\end{pmatrix}. \]
This reduces $A$ to
\[ L^{-1}A =
\begin{pmatrix}
  \vrule & \vrule & & \vrule & 
  1 \\
  \vrule & \vrule & & \vrule & \vspace{-0.5em}
  0 \\
  L^{-1}a_1 & L^{-1}a_2 & \dots & L^{-1}a_{n} & \vdots \\
  \vrule & \vrule & & \vrule & 0 \\
  \vrule & \vrule & & \vrule & 0
\end{pmatrix}, \]
and going a bit further we can see that
$(L^{-1}a_1,\dots,L^{-1}a_{n}) \in Y_{m,n}$ by computing
\[ L^{-1}\cdot \left(\Id,a_1,\dots,a_{n+1},\Id\right) =
\left(L^{-1},L^{-1}a_1,\dots,L^{-1}a_{n},
  \begin{array}{cccccc}
    1 &  1 &  0 & \dots &  0 & 0 \\
    0 & -1 &  1 & \dots &  0 & 0 \\
    0 &  0 & -1 & \dots &  0 & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 &  0 & 0 & \dots &  1 & 0 \\
    0 &  0 & 0 & \dots & -1 & 1
    % 1 & 1 & 0 & \dots & \dots & 0 \\
    % 0 & -1 & 1 & \dots & \dots& 0\\
    % 0 & 0 & -1 & \ddots & & \vdots\\
    % \vdots & \vdots & & \ddots & 1 & 0 \\
    % 0 & 0 & \dots & & -1& 1
  \end{array}
\right). \]
This shows that $L^{-1}\cdot(a_1,\dots,a_{n})$ is in
$Y_{m,n}(L^{-1},U)$, where $U$ is the
upper-triangular matrix given above. But this space is the same as
$Y_{m,n}$ since the right flag of a lower-triangular matrix and the
left flag of an upper-triangular matrix
is the same as the corresponding flag of the identity
matrix, as we saw in Section \ref{sec:flags}.

Since the above is done by multiplying with a matrix, it commutes with
the action of the torus $\T^n$ on $Y_{m,n}$, and everything can be
repeated with $Y_{m,n}$ and $Y_{m,n+1}$ replaced by the spaces
$X_{m,n}$ and $X_{m,n+1}$.

The above considerations allow us define a map that relates the spaces
$X_{m,n}$ and $X_{m,n+1}$, by working with the inverse matrix $L$.
\begin{definition}
  \label{def:stabilisering}
  The \textit{stabilisation map} $s : X_{m,n} \to X_{m,n+1}$ is given
  by
  \[ s\left(a_1,\dots,a_{n}\right) =
  \left(La_1,\dots,La_{n},Le_1\right), \]
  where $L$ is the lower-triangular matrix with every entry below the
  diagonal equal to one,
  \[ L =
  \begin{pmatrix}
    1 & 0 & 0 & \dots & 0 \\
    1 & 1 & 0 & \dots & 0 \\
    1 & 1 & 1 & \dots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & 1 & 1 & \dots & 1
  \end{pmatrix}. \]
\end{definition}

In the above, we only considered the spaces $X_{m,n}$ and $X_{m,n+1}$
defined by the identity permutation $\Id\in S_m$,
but there is a similar stabilisation for the other permutations. We
will again start by considering the unreduced spaces
$Y_{m,n}(\sigma)$. For
any permutation $\sigma$, the space $Y_{m,n+1}(\sigma)$ is a union of
spaces homeomorphic to a torus times $Y_{m,n}(\tau)$, for various
permutations $\tau\in S_m$. The exact relation is given by the
following theorem.

\begin{theorem}
  \label{thm:permutation}
  Any choice of indices $I= (i_1<\dots<i_k) \subset \set{1,\dots,m}$
  defines a subspace of $Y_{m,n+1}(\sigma)$:
  \[ Y_{m,n+1}^I(\sigma) = \set{(a_1,\dots,a_{n+1}) \in
    Y_{m,n+1}(\sigma) \sdel 
    \begin{matrix} 
      (a_{n+1})_{i_j} \neq 0\; \forall i_j \in I, \\
      (a_{n+1})_j = 0 \;\forall j \not\in I
    \end{matrix} }. \]
  Here $(a_{n+1})_j$ denotes the $j$'th entry of the vector $a_{n+1}$.
  This space is empty if $I$ does not contain the number $\sigma(m)$,
  so we will assume that $\sigma(m) \in I$ but otherwise leave it out
  of the notation.
  
  These subspaces are disjoint and cover $Y_{m,n+1}(\sigma)$. There is
  a map $\varphi$,
  defined for an indexing set $I$ and a permutation $\sigma$, which
  gives a new permutation such that
  \[ Y_{m,n+1}^I(\sigma) \cong Y_{m,n}(\varphi(I,\sigma)) \times
  (\C^*)^{\size{I}}. \]
  If we define the permutation $\widehat \sigma = \sigma \cdot
  \big(m\; m-1\; \dots \; 1\big)$ and the indexing set is ${I = (i_1 <
    \dots < i_k)}$, then $\varphi$ is given by
  \[ \varphi\big(I,\sigma\big) = 
  \begin{cases}
    \begin{matrix} \widehat\sigma \end{matrix} & 
    \begin{array}{l} k = 0, \end{array} \\
    \begin{matrix}
      \big( i_k\; \sigma(m) \big) \cdot \varphi\left( (i_1 < \dots <
        i_{k-1}),\sigma\right) \\
    \end{matrix} &
    \begin{array}{l} i_k < \sigma(m) \text{ and } \\
      \widehat\sigma^{-1}(i_k) > \widehat\sigma^{-1}(i_r) \,
      \forall r < k,\end{array}\\
    \begin{matrix}
      \varphi\left( (i_1 < \dots < i_{k-1}),\sigma\right)
    \end{matrix} &
    \begin{array}{l} \text{otherwise}. \end{array}
  \end{cases} \]
  Note that the index $\sigma(m)$ has once again been removed from the
  notation, 
  so the $k = 0$ case is really $I = (\sigma(m))$ and $i_k$
  is the last index not equal to $\sigma(m)$.
\end{theorem}
\begin{proof}
  Fix the permutation $\sigma \in S_m$. The observation that
  $Y^I_{m,n+1}(\sigma)$ is empty 
  if $\sigma(m)$ is not in $I$ follows from considering the matrix
  \[ (a_{n+1},\sigma_1,\dots,\sigma_{m-1}) =
  (a_{n+1},e_{\sigma(1)},\dots,e_{\sigma(m-1)}). \]
  For this matrix to be invertible we must have $(a_{n+1})_{\sigma(m)}
  \neq 0$. If $\sigma(m)$ is not in $I$, then the conditions on
  $Y^{I}_{m,n+1}(\sigma)$ can never be satisfied and the space must be
  empty. We will from now on assume that $I$ contains $\sigma(m)$.
  
  To see that the spaces are disjoint and cover all of
  $Y_{m,n+1}(\sigma)$, we can see that an element
  $(a_1,\dots,a_{n+1})\in Y_{m,n+1}(\sigma)$ is in exactly one
  subspace, namely the one corresponding to $I = \set{ j \in
    \set{1,\dots,m} \mid (a_{n+1})_j \neq 0}$.
  
  To define the function $\varphi$ and the homeomorphism
  \[ Y^I_{m,n+1}(\sigma) \cong Y_{m,n}(\varphi(I,\sigma))\times
  (\C^*)^{\size{I}}, \]
  we will proceed as in the previous case. The map to the torus
  $(\C^*)^{\size{I}}$ is given by remembering the non-zero entries of
  $a_{n+1}$, so we can immediately reduce to the case where all
  entries of $a_{n+1}$ are either zero or one.

  An element of $Y_{m,n+1}^I(\sigma)$ consists of vectors
  $(a_1,\dots,a_{n+1})$ with the matrix
  \[ M = (a_{n+1},\sigma_1,\dots,\sigma_{m-1}) \]
  invertible. It is this matrix that we want to factor as
  $L\cdot \varphi(I,\sigma)\cdot U$, where $L$ is an invertible,
  lower-triangular matrix and $U$ is an invertible, upper-triangular
  matrix. To do this, we want to make row operations on the matrix
  corresponding to multiplying with $L^{-1}$ until we end up with an
  upper-triangular matrix with permuted rows, corresponding to
  $\varphi(I,\sigma) \cdot U$. Multiplying with $L^{-1}$ corresponds
  to making row operations where each row is only allowed to affect
  the rows below it. We want to remove all non-zero entries in the
  first column except for one, and we want this single entry to have
  the lowest possible index. We also want to make the matrix as close
  as possible to upper-triangular, which is done by making sure that
  each operation does not introduce a non-zero entry to the left of an
  already existing non-zero entry.
  
  If $k = 0$, an element $Y^I_{m,n+1}(\sigma)$ looks like
  \[ (a_1,\dots,a_n,\sigma_m). \]
  Since we are in $Y_{m,n+1}(\sigma)$, the linear independence
  condition gives us that
  \[ (a_1,\dots,a_n) \in Y_{m,n}(\tau), \]
  where $\tau$ is the permutation
  \[ \tau(k) =
  \begin{cases}
    \sigma(k-1) & k > 1, \\
    \sigma(m) & k = 1.
  \end{cases} \]
  We recognize this as the permutation $\widehat\sigma$ and get the
  formula
  \[ \varphi((\sigma(m)),\sigma) = \widehat\sigma. \]
  The homeomorphism is in this case given by
  \begin{align*}
    Y_{m,n+1}^{(\sigma(m))}(\sigma) &\to Y_{m,n}(\widehat\sigma)\times
                                      \C^* \\
    (a_1,\dots,a_n,\lambda\cdot\sigma_m) &\mapsto
                                           \left((a_1,\dots,a_n),
                                           \lambda\right).
  \end{align*}
  
  Now consider the case $k > 0$, so $I = (i_1 < \dots < i_k)$. If $i_k
  > \sigma(m)$, we can write
  $(a_{n+1},\sigma_1,\dots,\sigma_{m-1})$ in block form as
  \[
  \begin{pmatrix}
    I_1 & A \\
    1 & 0 \\
    I_2 & B \\
    1 & C \\
    0 & D
  \end{pmatrix}.
  \]
  $I_1$ is a vector with ones on the entries corresponding
  to $\set{i_j \mid i_j < \sigma(m)}$ and zeroes elsewhere, while
  $I_2$ is similar except we consider indices greater than
  $\sigma(m)$. The first 1 is on the $\sigma(m)$'th row and the next
  is on the $i_k$'th row. All entries below are zero. $A$, $B$, $C$
  and $D$ are the matrices consisting of the corresponding rows from
  $(\sigma_1,\dots,\sigma_{m-1})$. But by multiplying with a
  lower-triangular matrix, we can reduce this matrix to
  \[
  \begin{pmatrix}
    I_1 & A \\
    1 & 0 \\
    I_2 & B \\
    0 & C \\
    0 & D
  \end{pmatrix}.
  \]
  But this shows that $\varphi(I,\sigma) =
  \varphi\big((i_1<\dots<i_{k-1}),\sigma\big)$ when $i_k >
  \sigma(m)$.

  By possibly using the above multiple times, we reduce to the case
  where $i_k < \sigma(m)$. Now we would like to reduce the length of
  $I$ by one, and consider how this affects the resulting permutation.
  Consider $\widehat\sigma^{-1}(i_k)$. If
  there is a $j < k$ with $\widehat\sigma^{-1}(i_k) <
  \widehat\sigma^{-1}(i_j)$, then we can write
  $(a_{n+1},\sigma_1,\dots,\sigma_{m-1})$ as
  \[
  \begin{pmatrix}
    \vdots & \\
    1 & \dots & \dots & \dots & 1 & \dots\\
    \vdots \\
    1 & \dots & 1 & \dots & \dots & \dots\\
    \vdots & \\
    1 &  \dots & \dots & \dots & \dots & \dots\\
    \vdots
  \end{pmatrix}. \]
  In the above, only the $i_j$'th, $i_k$'th and $\sigma(m)$'th rows
  have been written out, and only the non-zero entries. But this
  matrix can be reduced to
  \[
  \begin{pmatrix}
    \vdots & \\
    1 & \dots & \dots & \dots & 1 & \dots\\
    \vdots \\
    0 & \dots & 1 & \dots & -1 & \dots\\
    \vdots & \\
    1 & \dots & \dots  & \dots & \dots & \dots\\
    \vdots
  \end{pmatrix}. \]
  We are now unable to change the middle row, row $i_k$, since there
  are no non-zero
  entries above the left-most one. The additional $-1$ that was
  introduced can be removed by multiplying on the right with an
  upper-triangular matrix that subtracts column
  $\widehat\sigma^{-1}(i_k)$
  from column $\widehat\sigma^{-1}(i_j)$. So we are able to remove the
  one in the $i_k$'th row without changing the resulting permutation,
  showing that in this case,
  \[ \varphi\big(I,\sigma\big) =
  \varphi\big((i_1<\dots<i_{k-1}),\sigma\big). \]

  Now assume that there is no such $j$. Then the row $i_k$ is our best
  candidate for removing the bottom-most one in row $\sigma(m)$. If we
  do this row operation, by multiplying with $\widehat L$, and then
  use the
  Bruhat decomposition, we get
  \[ \varphi(I,\sigma)\cdot U = \widetilde{L}^{-1}\widehat{L} \cdot
  M. \]
  But when we perform row operations on $\widehat{L} \cdot M$, we are
  only changing rows with index less than or equal to $i_{k-1}$, since
  this is the last non-zero entry in the first column. Hence
  $\widetilde{L}^{-1}$
  has the form
  \[ \widetilde{L}^{-1} = 
  \begin{pmatrix}
    \vdots \\
    * & \dots & * & 1 & \dots & \dots & \dots \\
    \vdots \\
    0 & \dots & \dots & \dots & \dots & 1 & \dots \\
    \vdots
  \end{pmatrix},
  \]
  where we once again write out row $i_k$ and $\sigma(m)$, and use the
  stars to indicate entries that may be non-zero but that are not
  important.
  If we switch row $i_k$ and row $\sigma(m)$ along with column
  $i_k$ and column $\sigma(m)$, we get another lower-triangular
  matrix:
  \[ (i_k \; \sigma(m)) \widetilde{L}^{-1} (i_k \; \sigma(m)) = 
  \begin{pmatrix}
    \vdots \\
    0 & \dots & \dots & 1 & \dots & \dots & \dots \\
    \vdots \\
    * & \dots & * & \dots & \dots & 1 & \dots \\
    \vdots
  \end{pmatrix}.\]
  Inserting in our original equation, we see
  \[ (i_k \; \sigma(m)) \varphi(I,\sigma) \cdot U = \widetilde{L}^{-1}
  \cdot (i_k \; \sigma(m)) \cdot \widehat{L} M. \]
  But $(i_k \; \sigma(m)) \cdot \widehat{L} M$ is just $M$ with the
  one in the first column, row $i_k$, replaced by zero. This gives the
  formula
  \[ (i_k \; \sigma(m)) \cdot \varphi(I,\sigma) =
  \varphi\big((i_1<\dots<i_{k-1}),\sigma\big), \]
  and our desired formula for $\varphi$ follows by multiplying over.
\end{proof}

By looking at the formula for $\varphi$, we can deduce the following
theorem that shows how the different possible permutations that can
show up are related in the Bruhat order, described in Section
\ref{sec:bruhat}.

\begin{theorem}
  \label{thm:bruhat-ord}
  If $I = (i_1 < \dots < i_k)$ and $J$ is a subset of $I$, then the
  permutation $\varphi(I,\sigma)$ is less than or equal to the
  permutation $\varphi(J,\sigma)$ in the Bruhat order.
\end{theorem}
A different way of phrasing this is that as the indexing set becomes
larger, we get closer to $\varphi\left((1 < 2 < \dots <
  m),\sigma\right)$ and further away from
$\varphi\left((\sigma(m)),\sigma\right)$.
\begin{proof}
  The proof is mostly checking cases. If $\varphi(J,\sigma) =
  \varphi(I,\sigma)$ we are done. We will start by considering the
  simplest remaining case, where
  \[ \varphi(I,\sigma) = (i_k \; \sigma(m)) \cdot 
  \varphi(J,\sigma). \]
  First we note that the two permutations are related in the Bruhat
  order, since
  \[ \tau^{-1} \cdot \phi = \varphi(J,\sigma)^{-1} \cdot
  (i_k\;\sigma(m)) \cdot \varphi(J,\sigma), \]
  which is exactly the condition described in Definition
  \ref{def:bruhat-def}. So
  we only need to count the number of inversions of each of the
  permutations, and
  see that $\tau$ has fewer inversions than $\phi$.
  To simplify further, assume that all the indices $i_r$ in $I$ and
  $J$ satisfy the conditions $i_r < \sigma(m)$ and
  $\widehat{\sigma}^{-1}(i_r) > \widehat{\sigma}^{-1}(i_s)$ for all $s
  < r$. This corresponds to removing the entries of $I$ that have no
  effect on $\varphi(I,\sigma)$ by the previous theorem. Then we can
  compute the two permutations we are interested in by using the
  formula for $\varphi$,
  \begin{align*}
    \tau &= \varphi(I,\sigma) = (i_1\;\dots\;i_{k-1}\;i_k\;\sigma(m))
           \cdot \widehat{\sigma}, \\
    \phi &= \varphi(J,\sigma) = (i_1\;\dots\;i_{k-1}\;\sigma(m)) \cdot
    \widehat{\sigma}.
  \end{align*}

  From the
  definition of $\tau$ and $\phi$, we can see that if $a$ is not in
  the set
  \[
  \set{\widehat{\sigma}^{-1}(i_{k-1}),\widehat{\sigma}^{-1}(i_{k})},
  \]
  then $\tau(a) = \phi(a)$. We would like to find pairs $(a,b)$
  with $a < b$ that are inversions for eiher $\tau$ or $\phi$, but not
  both. So we should only consider pairs $(a,b)$ where at least one is
  either $\widehat{\sigma}^{-1}(i_k)$ or
  $\widehat{\sigma}^{-1}(i_{k-1})$.

  Note that by our previous assumption on the indices $i_r$, we have 
  $\widehat{\sigma}^{-1}(i_{k-1}) < \widehat{\sigma}^{-1}(i_{k})$. So
  we start by checking this pair.
  \begin{align*}
    \tau(\widehat{\sigma}^{-1}(i_{k-1})) = i_k <\,& \sigma(m) =
    \tau(\widehat{\sigma}^{-1}(i_{k})), \\
    \phi(\widehat{\sigma}^{-1}(i_{k-1})) = \sigma(m) &> i_k =
    \phi(\widehat{\sigma}^{-1}(i_{k})).
  \end{align*}
  This is an inversion for $\phi$, but not for $\tau$. 

  Now we consider
  pairs $(a,b)$ that are inversions for $\tau$ but not for $\phi$, and
  in which exactly one of the two numbers are in the set
  $\set{\widehat{\sigma}^{-1}(i_{k-1}),\widehat{\sigma}^{-1}(i_{k})}$.
  These satisfy the inequalities
  \begin{align*}
    a &< b, \\
    \tau(a) &> \tau(b), \\
    \phi(a) &< \phi(b).
  \end{align*}
  If $a = \widehat{\sigma}^{-1}(i_{k-1})$, then the inequalities
  reduce to
  \[ i_k = \tau(a) > \tau(b) = \phi(b) > \phi(a) = \sigma(m), \]
  which is impossible since $i_k < \sigma(m)$ by assumption.
  If $b = \widehat{\sigma}^{-1}(i_{k})$ we get
  \[ \sigma(m) = \tau(b) < \tau(a) = \phi(a) < \phi(b) = i_k, \]
  which is again impossible. So neither of these cases give rise to
  inversions for $\tau$.
  If $a = \widehat{\sigma}^{-1}(i_{k})$, the inequalities become
  \[ i_k < \phi(b) = \tau(b) < \sigma(m). \]
  This gives us a set of inversions of the form
  \[ \set{(\widehat{\sigma}^{-1}(i_{k}),s) \mid s >
    \widehat{\sigma}^{-1}(i_{k}), i_k < \phi(s) < \sigma(m)}. \]
  If $b = \widehat{\sigma}^{-1}(i_{k-1})$, we get
  \[ i_k < \tau(a) = \phi(a) < \sigma(m), \]
  and a set of inversions of the form
  \[ \set{(s,\widehat{\sigma}^{-1}(i_{k-1})) \mid s <
    \widehat{\sigma}^{-1}(i_{k-1}), i_k < \phi(s) < \sigma(m)}. \]

  Now we consider inversions for $\phi$ that are not inversions for
  $\tau$. These satisfy
  \begin{align*}
    a &< b, \\
    \tau(a) &< \tau(b), \\
    \phi(a) &> \phi(b).
  \end{align*}
  If $a = \widehat{\sigma}^{-1}(i_{k})$, we get
  \[ \sigma(m) < \tau(b) = \phi(b) < i_k, \]
  which is impossible. Likewise, $b = \widehat{\sigma}^{-1}(i_{k-1})$
  gives
  \[ \sigma(m) < \phi(a) = \tau(a) < i_k. \]
  So these cases contribute nothing. If $a$ is
  $\widehat{\sigma}^{-1}(i_{k-1})$, then
  \[ i_k < \tau(b) = \phi(b) < \sigma(m) \]
  and we get the set
  \[ \set{(\widehat{\sigma}^{-1}(i_{k-1}),s) \mid s >
    \widehat{\sigma}^{-1}(i_{k-1}), s \neq
    \widehat{\sigma}^{-1}(i_{k}), i_k < \phi(s) < \sigma(m)}. \]
  The last remaining case is $b = \widehat{\sigma}^{-1}(i_{k})$, which
  gives
  \[ i _k < \phi(a) = \tau(a) < \sigma(m), \]
  with inversions
  \[ \set{ (s,\widehat{\sigma}^{-1}(i_{k})) \mid s <
    \widehat{\sigma}^{-1}(i_{k}), s \neq
    \widehat{\sigma}^{-1}(i_{k-1}), i_k < \phi(s) < \sigma(m) }. \]
  
  But if we consider the sets of inversions, we can see that all the
  new inversions for $\tau$ have a corresponding inversion for
  $\phi$. So since we certainly removed the inversion
  $(\widehat{\sigma}^{-1}(i_{k-1}),\widehat{\sigma}^{-1}(i_{k}))$ when
  we passed from $\phi$ to $\tau$, we have not added new ones without
  also removing old inversions. All in all, this shows that
  $\varphi(I,\sigma) = \tau$ has fewer inversions than
  $\varphi(J,\sigma) = \phi$, giving the desired inequality in the
  Bruhat order,
  \[ \varphi(I,\sigma) \leq \varphi(J,\sigma). \]
  To get the general case, note that the Bruhat order is transitive
  by definition, so we simply apply the above argument several times.
\end{proof}

A summary of the above theorem is that the map $\varphi(\cdot,\sigma)$
is order-reversing when we order the indexing sets by inclusion.

Now that we have a stabilisation map, we can give explicit elements of
$Y_{m,n}(\sigma)$, as was hinted at earlier.

\begin{theorem}
  \label{thm:eksplicit-element}
  If the space $Y_{m,n}(\sigma)$ is non-empty, then the sequence of
  vectors
  \[ s^n(e_1) = \left(L^n e_1,\dots,Le_1\right) \]
  belongs to $Y_{m,n}(\sigma)$.
\end{theorem}
\begin{proof}
  For a discussion of when $Y_{m,n}(\sigma)$ is non-empty, consult
  Theorem \ref{thm:ikke-tom}.
  
  We already know that the sequence
  \[ \left(e_1,\dots,e_m,L^n e_1,\dots, Le_1 \right) \]
  has all subsequent sequences of $m$ vectors linearly independent,
  by the construction of the stabilisation map. So the thing to check
  is whether the matrices
  \[ \left(e_{k+n+1}, \dots, e_m,L^n e_1, \dots, Le_1, \sigma_1, \dots,
    \sigma_{k}\right) \]
  are invertible, for $k \in \set{1,\dots,m-1}$. If $n+k$ is greater
  than or equal to $m$, we of course mean the matrices
  \[ \left( L^{m-k}e_1,\dots,L e_1,\sigma_1,\dots,\sigma_k \right). \]
  
  Calculating the determinant of a matrix of this form by column
  expansion, we see that expanding along the column $\sigma_i$ removes
  row $\sigma(i)$ from the matrix and introduces a sign. So this
  matrix being invertible for all $\sigma$ is equivalent to the matrix 
  \[ R = 
  \begin{pmatrix}
    \matrow{r_{i_1}} \\
    \matrow{r_{i_2}} \\
    & \hspace{-0.75em} \vdots & \\
    \matrow{r_{i_s}}
  \end{pmatrix}
  \]
  being invertible for any choice
  of $s=m-k$ numbers,
  $i_1,\dots,i_s \in \set{1,\dots,m},$ with $i_j < i_{j+1},$ 
  where $r_{l}$ denotes the $l$'th row of the
  matrix
  \[ P = (L^{m-k}e_1,\dots,Le_1). \]
  
  Calculating $P$ by induction, we see that it is related to
  Pascal's triangle,
  \[ P =
  \begin{pmatrix}
    p_{i,j}
  \end{pmatrix}
  =
  \begin{pmatrix}
    {m-k-1 \choose 0}\vspace{1em} & {m-k-2\choose 0} & \dots &
    {1\choose 0} & {0 \choose 0} \\
    {m-k \choose 1} \vspace{0.5em} & {m-k-1 \choose 1} & \dots & {2
      \choose 1} & {1 \choose 1} \\
    \vdots \vspace{0.5em} & \vdots & \ddots & \vdots & \vdots\\
    {2(m-k-1) \choose {m-k-1}} & {2(m-k-1)-1 \choose m-k-1} & \dots &
    {m-k \choose m-k-1} & {m-k-1 \choose m-k-1}
  \end{pmatrix}, \]
  with the entry in the $i$'th row and $j$'th column given by the
  formula
  \[ p_{i,j} = { m-k-j+i-1 \choose i-1}. \] 
  We start with column $j=m-k$. This column is always $Le_1$, which we
  know consists of all ones, so the formula holds here since
  \[ p_{i,m-k} = { m-k-(m-k)+i-1 \choose i-1} = {i-1\choose i-1}. \] 
  Now assume that the formula holds for column
  $j$, given as $L^{m-k-j+1}e_1$. Moving a column to the left is the
  same as multiplying with $L$, which corresponds to
  adding all of the entries above a given entry together. So we get
  \[ p_{i,j-1} = \sum_{r=1}^{i} p_{r,j}  =  \sum_{r=1}^i { m-k-j+r-1
    \choose r-1}.  \]
  But the entry $p_{1,j}$ is always one, so we can replace
  $p_{1,j}$ with $p_{1,j-1}$ in the sum without changing the
  result. The conclusion then follows from the standard formulas for
  binomial coefficients,
  \begin{align*}
    p_{2,j-1} = p_{1,j} + p_{2,j} 
    = p_{1,j-1} + p_{2,j} &= {m-k-j+1\choose 0} + {m-k-j+1 \choose
                            1}\\
                          &= {m-k-j+2\choose 1}\\
                          &= {m-k-(j-1)+2-1\choose 2-1},
  \end{align*}
  and we can continue like this to get the formula for $p_{i,j}$
  given above.
  
  In particular, by the standard formulas for binomial coefficients,
  \[ p_{i,j} = p_{i-1,j} + p_{i,j+1}, \]
  so the row $r_{j}$ can be calculated by taking the row
  above,
  $r_{j-1}$, and adding the row $\widehat{r}_j$ given by shifting
  every entry of $r_j$ one to the left and adding a zero,
  \[ r_j = r_{j-1} + \widehat{r}_j. \]
  To illustrate, if $m-k$ is three, then $P$ is the matrix
  \[ P = 
  \begin{pmatrix}
    1 &1 & 1 \\
    3 &2 & 1 \\
    6 & 3 & 1
  \end{pmatrix} \]
  and the third row is given by
  \[
  \begin{pmatrix}
    6 &3 & 1
  \end{pmatrix} = r_3 = r_2 + \widehat{r}_3 =
  \begin{pmatrix}
    3 & 2 & 1
  \end{pmatrix} +
  \begin{pmatrix}
    3 & 1 & 0
  \end{pmatrix} \]
  
  We will prove that the matrix
  \[ R = 
  \begin{pmatrix}
    \matrow{r_{i_1}} \\
    \matrow{r_{i_2}} \\
    & \hspace{-0.75em} \vdots & \\
    \matrow{r_{i_s}}
  \end{pmatrix}
  \]
  is invertible by induction on $s$. The exact statement we intend to
  prove is that the determinant of $R$ is strictly positive if two
  divides $s$ an even number of times and is strictly negative if
  two divides $s$ an odd number of times.
  For $s = 1$, $R$ is always the matrix $R = \left( 1 \right)$ and we
  are done since $\det R = 1$ is strictly positive and two divides $s$
  zero times.
  Assume the statement is correct for $s-1$. Since the determinant is
  linear in each row, we can use the equation for row $r_j$ given
  above and get
  \begin{align*}
    \det{R} &= 
              \det\begin{pmatrix}
                \matrow{r_{i_1}} \\
                \matrow{r_{i_2}} \\
                & \hspace{-0.75em} \vdots & \\
                \matrow{r_{i_{(s-1)}}} \\
                \matrow{r_{i_s}}
              \end{pmatrix} \\
            &= 
              \det\begin{pmatrix}
                \matrow{r_{i_1}} \\
                \matrow{r_{i_2}} \\
                & \hspace{-0.75em} \vdots & \\
                \matrow{r_{i_{(s-1)}}} \\
                \matrow{r_{i_s-1}}
              \end{pmatrix}+
    \det\begin{pmatrix}
      \matrow{r_{i_1}} \\
      \matrow{r_{i_2}} \\
      & \hspace{-0.75em} \vdots & \\
      \matrow{r_{i_{(s-1)}}} \\
      \matrow{\widehat{r}_{i_s}}
    \end{pmatrix} \\
            &=   \det\begin{pmatrix}
              \matrow{r_{i_1}} \\
              \matrow{r_{i_2}} \\
              & \hspace{-0.75em} \vdots & \\
              \matrow{r_{i_{(s-1)}}} \\
              \matrow{r_{i_{(s-1)}}}
            \end{pmatrix}
    + \sum_{j=0}^{i_s-i_{(s-1)}-1} \det\begin{pmatrix}
      \matrow{r_{i_1}} \\
      \matrow{r_{i_2}} \\
      & \hspace{-0.75em} \vdots & \\
      \matrow{r_{i_{(s-1)}}} \\
      \matrow{\widehat{r}_{i_s-j}} \end{pmatrix}.
  \end{align*}
  The first determinant is zero, since the matrix has two equal rows,
  so we are left with the sum. Doing the same thing for all the other
  rows except for the first one gives us
  \[ \det R = \sum_{j_1 = 0}^{i_2-i_1-1}\sum_{j_2 = 0}^{i_3-i_2-1}
  \dots \sum_{j_{(s-1)}=0}^{i_s-i_{(s-1)}-1} \det
  \begin{pmatrix}
    \matrow{r_{i_1}} \\
    \matrow{\widehat{r}_{i_2-j_1}} \\
    & \hspace{-0.75em} \vdots & \\
    \matrow{\widehat{r}_{i_{(s-1)}-j_{(s-2)}}} \\
    \matrow{\widehat{r}_{i_s-j_{(s-1)}}}
  \end{pmatrix}. \]
  
  By considering the last column of one of these matrices, we see that
  it has the form
  \[
  \begin{pmatrix}
    1 \\
    0 \\
    \vdots \\
    0
  \end{pmatrix} \]
  since we have shifted every row below the first one space to the
  left. By expanding the determinant using this column, we remove the
  first row and the last column, possibly introduce a sign, and
  end up with
  \[ \det R = \sum_{j_1 = 0}^{i_2-i_1-1}\sum_{j_2 = 0}^{i_3-i_2-1}
  \dots \sum_{j_{(s-1)}=0}^{i_s-i_{(s-1)}-1} (-1)^{s-1} \det
  \begin{pmatrix}
    \matrow{\widetilde{r}_{i_2-j_1}} \\
    & \hspace{-0.75em} \vdots & \\
    \matrow{\widetilde{r}_{i_{(s-1)}-j_{(s-2)}}} \\
    \matrow{\widetilde{r}_{i_s-j_{(s-1)}}}
  \end{pmatrix}. \]
  Here we use $\widetilde{r}_i$ to denote the $i$'th row of $P$ where we
  have removed the leftmost entry. But we can now apply the induction
  hypothesis. We see that all the determinants have the same sign and
  are non-zero,
  so $\det R$ is non-zero with opposite sign if $s-1$ is odd and
  with the
  same sign if $s-1$ is even. This exactly means that $\det R$ changes
  sign
  when $s$ passes a multiple of two, as stated above, and we are done.
\end{proof}

\section{The limit space}
\label{sec:rum-gr}

Using the stabilisation defined previously, we get a directed system
of topological spaces,
\[ \xymatrix{X_{m,1} \ar[r]^s & X_{m,2} \ar[r]^s & \dots \ar[r]^s &
  X_{m,n} \ar[r]^s & X_{m,n+1} \ar[r]^s & \dots } \]
When considering such a system, it can sometimes be helpful to
take the limit as $n$ approaches infinity.

\begin{definition}
  The space $X_{m,\infty}$ is defined as the direct limit of the
  directed system of spaces,
  \[ X_{m,\infty} = \varinjlim_n X_{m,n}. \]
  As a set, $X_{m,\infty}$ is the disjoint union of all the spaces,
  where we identify $A \in X_{m,k}$ with $B \in X_{m,k'}$ if they
  eventually map to the same thing under $s$:
  \[ A \sim B \iff \exists\, i,j : s^i(A) = s^j(B). \]
  We give $X_{m,\infty}$ the finest topology such that all the
  maps 
  \[ \imath_n : X_{m,n} \to X_{m,\infty} \]
  are continuous. So a subset $U \subset X_{m,\infty}$ is open if and
  only if $\imath_n^{-1}(U)$ is open in $X_{m,n}$ for all n.
\end{definition}

It is this limit space that we will focus on in the following
chapters, especially in Chapter \ref{chap:loekker}. Before we do this,
we will however need to set up some tools, which will be done in the
next chapter.

\section{Quaternionic and real cases}
\label{sec:koeff}

When the spaces $Y_{m,n}(X,Y)$ were introduced in Definition
\ref{def:rum}, we could have used other entries for our vectors than
the complex numbers. Two possible alternatives would be the real
numbers, $\R$, or the quaternions, $\Hq$, giving us the spaces
$Y^{\R}_{m,n}(X,Y)$ and $Y^{\Hq}_{m,n}(X,Y)$. These behave much like
the spaces studied above, with the notable exception that since the
quaternions are not commutative one needs to choose whether to act
with scalars from the left or from the right. Since the stabilisation
map,
\[ s : Y_{m,n}^{\Hq} \to Y_{m,n+1}^{Hq}, \]
acts on the left with a matrix and this map should be linear to give
an induced map on the quotient $X^{\Hq}_{m,n}$, we will choose to act
with scalars from the right. Once this is done, everything above
carries out exactly as for the complex numbers, and we get
stabilisation maps and limit spaces exactly as for the complex
case. These will not be important in the next chapter, but will show
up again in Chapter \ref{chap:loekker}.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
