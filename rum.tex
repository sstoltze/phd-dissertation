


% Skal der laves et intro-kapitel med baggrundsmateriale? Løkker
% (Milnor), Bruhat, spektralfølger.
% Ellers skal det nok ind her et sted, men måske spredt lidt ud over
% kapitlet.

\chapter{Spaces of linearly independent vectors}
\fixme{Bedre titel?}
\fixme{Sæt memoir til at bruge den rigtige chapter-overskrift så det
  ikke ligner lort.}

% Introduktion til rum
%% Ukvotienterede (Y_{m,n}) og de rigtige (X_{m,n} = Y_{m,n}/T^m)
%%% Så introducer gruppevirkningen her. Og vis at Y \cong X x T
%%% Bør nok også nævne virkningen på rækkerne, da den spiller ind i
%%% spektralfølgen.
%% Andre permutationer.
%% Andre koefficienter: \R og \H. Disse dukker først op igen
%% senere, men det meste af teorien i dette afsnit burde være
%% uafhængigt af koefficienterne, op til at nogle skalarer flytter
%% lidt rundt. Specielt kan vi lige så godt smide dem ind her.

In this chapter we will introduce the spaces that will be studied in
the rest of the document, and collect various useful
results.\fixme{Bleh. Skriv mere}

\section{The spaces} \fixme{Bedre overskrift...}
\label{sec:rum}
Before we can define the spaces that we will be working with, it will
be helpful to define an approximation to them.

\begin{definition} 
  For natural numbers $m$ and $n$, the space $Y_{m,n}\subset \C^{mn}$
  is \fixme{Hvor skal $\R$ og $\Hq$ introduceres?}
  \[ Y_{m,n} = \set{(a_1,\dots,a_n) \in (\C^m)^n \delim\Bigg\vert\delim
    \begin{matrix}
      \text{Any } m \text{ subsequent vectors in } \\
      (e_1,\dots,e_m,a_1,\dots,a_n,e_1,\dots,e_m) \\
      \text{ are linearly independent.}
    \end{matrix} }, \]
  where $e_1,\dots,e_m \in \C^m$ denotes the standard basis
  vectors,
  \[ e_i =
  \big(0,\dots,0,\overset{\substack{i\\\downarrow}}{1},0\dots,0\big).\]
\end{definition}

We will switch between considering $(a_1,\dots,a_n)\in Y_{m,n}$ as an
ordered sequences of linearly independent vectors and an $m\times n$
matrix $[a_1,\dots,a_n]$ as needed and without mention. We can replace
the standard basis that appears in the definition of $Y_{m,n}$ with
any collection of linearly independent vectors and get similarly
defined spaces: 
\begin{definition}
  \label{def:rum}
  For two invertible matrices $X$ and $Y$ in
  $\GL_m(\C)$, the space \newline
  $Y_{m,n}(X,Y) \subset \C^{mn}$ is
  \[ Y_{m,n}(X,Y) = \set{(a_1,\dots,a_n) \in (\C^m)^n
    \delim\Bigg\vert\delim 
    \begin{matrix}
      \text{Any } m \text{ subsequent vectors in } \\
      (x_1,\dots,x_m,a_1,\dots,a_n,y_1,\dots,y_m) \\
      \text{ are linearly independent.}
    \end{matrix} }, \]
  where $(x_1,\dots,x_m)$ and $(y_1,\dots,y_m)$ are the column vectors
  of $X$ and $Y$ respectively.
  
  There are two special cases that will be important.
  If both $X$ and $Y$ are the identity-matrix, we get the previously
  defined spaces,
  \[ Y_{m,n} = Y_{m,n}(\Id,\Id). \]
  When $X$ is the identity-matrix, we will usually omit it from the
  notation and instead write
  \[ Y_{m,n}(Y) = Y_{m,n}(\Id,Y). \]
\end{definition}

Note that $Y_{m,n}(X,Y)$ can also be defined as the
pre-image of the open set $(\C^*)^{m+n-1}$ in $\C^{m+n-1}$ under the
continuous map $\Det_{X,Y}$ that calculates determinants of subsequent
vectors:
\begin{align*}
  \Det_{X,Y} : (\C^m)^n \to &\;\C^{m+n-1} \\
  \Det_{X,Y}(a_1,\dots,a_n) = \,&\; (\det(x_2,\dots,x_m,a_1),\\
  &\; \det(x_3,\dots,x_m,a_1,a_2), \\
  &\;\;\;\; \vdots\\
  &\; \det(a_n,y_1,\dots,y_{m-1})).
\end{align*}
This shows that $Y_{m,n}(X,Y)$ is an open subset of
$(\C^m)^n$. Another important
feature of $Y_{m,n}(X,Y)$ is that it does not really depend on the
particular matrices
$X =\nobreak [x_1,\dots,x_m]$ and $Y = [y_1,\dots,y_m]$,
but only on the two flags in $\C^{m}$ defined from the columns:
\begin{alignat*}{5}
  \mathrm{Fl_R}(X)\, &&= \Big(&\spa(x_m) &&\subset \spa(x_{m-1},x_m)
  &&\subset \dots &&\subset \C^m\Big) \\
  \mathrm{Fl_L}(Y)\, &&= \Big(&\spa(y_1) &&\subset \spa(y_1,y_2) &&\subset
  \dots &&\subset \C^m\Big).
\end{alignat*}
This is because we are considering linear independence of the vectors
\[ (x_1,\dots,x_m,a_1,\dots,a_n,y_1,\dots,y_n), \]
and if $(a_1,\dots,a_{k})$ is a collection of linearly independent
vectors of $\C^m$, then the vectors
$(x_{k+1},\dots,x_m,a_1,\dots,a_{k})$ are
linearly independent if and only if $a_1$ is not in the subspace
spanned by $(x_{k+1},\dots,x_m)$, $a_1$ and $a_2$ are linearly
independent and not in the
subspace spanned by $(x_{k+2},\dots,x_m)$ and so forth. The same
argument can be applied to the columns of $Y$.

At first glance it looks like we are considering a huge collection of
spaces, but most of them are indistinguishable topologically, as the
following lemma shows.

\begin{lemma} \label{lem:rum-perm}
  For $X,Y\in \GL_m(\C)$, there exists a unique permutation $\sigma
  \in S_m$ such that the space
  $Y_{m,n}(X,Y)$ is homeomorphic to $Y_{m,n}(\sigma)$.
\end{lemma}

\begin{proof}
  Since $X$ is invertible it preserves linear independence of vectors,
  and we get a homeomorphism
  \begin{align*}
    Y_{m,n}(X,Y) &\to Y_{m,n}(\Id,X^{-1}Y) \\
    (a_1,\dots,a_m) &\mapsto (X^{-1}a_1,\dots,X^{-1}a_m).
  \end{align*}
  The matrix $X^{-1}Y$ is invertible, and by using the
  Bruhat-decomposition of $\GL_m(\C)$ we can write it uniquely as a
  product \fixme{Reference, enten til tidligere eller til bog}
  \[ X^{-1}Y = L\sigma U \]
  where $L$ is lower-triangular, $U$ is upper-triangular and $\sigma$
  is a permutation. Hence we get a homeomorphism,
  \[ Y_{m,n}(X,Y) \cong Y_{m,n}(L^{-1}, \sigma U). \]
  But by the remark above, the space $Y_{m,n}(L^{-1},\sigma U)$ only
  depends
  on the flags 
  \begin{align*}
    \mathrm{Fl_R}(L^{-1}) &= \mathrm{Fl_R}(\Id) \\
    \mathrm{Fl_L}(\sigma U) &= \mathrm{Fl_L}(\sigma).
  \end{align*}
  So $Y_{m,n}(L^{-1},\sigma U)$ is homeomorphic to the space $Y_{m,n}(\sigma)$.
\end{proof}

Due to this, we will almost exclusively consider the spaces
$Y_{m,n}(\sigma)$ throughout the text. To get a feel for the spaces,
we will now work with some simple examples.

\begin{example}
  \label{ex:n=1}
  The space $Y_{m,1}$ is homeomorphic to $(\C^*)^m$, since
  \[\begin{pmatrix}
    1&0&\dots&0&\l_1&1&0&\dots&0 \\
    0&1&\dots&0&\l_2&0&1&\dots&0 \\
    \vdots&\vdots&\ddots&\vdots&\vdots&\vdots&\vdots&\ddots&\vdots \\
    0&0&\dots&1&\l_m&0&0&\dots&1
  \end{pmatrix} \]
  has linearly independent columns if and only if all the scalars
  $\l_1,\dots,\l_m$ are invertible.
\end{example}

\begin{example}
  The space $Y_{2,2}$ is
  \[ \set{
    \begin{pmatrix}
      a & b \\
      c & d
    \end{pmatrix} \delim\bigg\vert\delim a \neq 0, d \neq 0, ad-bc
    \neq 0 } \cong (\C^*)^2 \times \set{(b,c)\in\C^2 \delim\vert\delim
    bc \neq 1}, \]
  with the homeomorphism given by
  \[
  \begin{pmatrix}
    a & b\\
    c & d
  \end{pmatrix} \mapsto \left(a,d,\frac{b}{d},\frac{c}{a}\right).\]
  The product of the two fractions is
  \[ \frac{bc}{ad} = \frac{bc-ad}{ad} + 1 = 1-\frac{ad-bc}{ad} \neq
  1, \]
  since $ad-bc \neq 0$.
\end{example}

\begin{example}
  The space $Y_{3,2}$ is
  \[\set{
    \begin{pmatrix}
      a &b \\
      c & d \\
      e & f
    \end{pmatrix} \delim\Bigg\vert\delim a \neq 0, f \neq 0, ad-bc \neq
    0, cf-ed \neq 0}.
  \]
  This is homeomorphic to $(\C^*)^4\times \set{(x,y,z)\in\C^3 \mid
    y-z-xyz = 1}$, with the map given by
  \[
  \begin{pmatrix}
    a &b \\
    c & d \\
    e & f
  \end{pmatrix} \mapsto
  \left(a, f, D=\frac{ad-bc}{af}, C=\frac{cf-de}{af}, x=\frac{bC}{fD},
    y=\frac{c}{aC}, z=\frac{eD}{aC} \right). \]
  Some of the coordinates have been named to shorten the writing
  slightly. By direct calculation,
  \begin{align*}
    y-z-xyz &= \frac{c}{aC}-\frac{eD}{aC}-\frac{bce}{a^2fC} \\
            &= \frac{acf-aefD-bce}{a^2fC} \\
            &= \frac{acf-e(ad-bc)-bce}{a(cf-de)}  \\
            &= \frac{acf-ade}{acf-ade} \\
            &= 1.
  \end{align*}
  The inverse map is
  \[ (a,f,D,C,x,y,z) \mapsto
  \begin{pmatrix}
    a & \frac{fDx}{C} \\
    aCy & fD(1+xy) \\
    \frac{aCz}{D} & f
  \end{pmatrix}. \]
\end{example}


% Vis at de er ikke-tomme
%% Nok både dimensions-argumentet og vise at stabiliseringen af e_1 er
%% i alle rummene.

To work with the above spaces, it will help to know that they are not
empty. For example, by considering the permutation $\big(1\; 3\big)$,
we see that it is impossible to satisfy the 
conditions for being in $Y_{3,1}\left(\big(1\; 3\big)\right)$. To see
this, take any vector $a_1$ in $\C^3$ and look at
$(e_1,e_2,e_3,a_1,e_3,e_2,e_1)$. If this is to be in the space, it
must have any three subsequent vectors linearly independent, but
$(e_3,a_1,e_3)$ is not invertible no matter what $a_1$ is. This shows
that the choice of $X$ and $Y$ in $Y_{m,n}(X,Y)$ plays a role in
determining when the spaces are empty. But as we shall now see, the
spaces are non-empty as long as we are not in an obviously empty
space. The exact meaning of ``obviously'' is given in the following
lemma.

\begin{theorem}
  \label{thm:ikke-tom}
  The space $Y_{m,n}(\sigma)$ is empty if and only if there exists an
  $i \in \set{1,\dots,m}$ such that
  \[ 1 + n + i \leq \sigma(i). \]
\end{theorem}
\begin{proof}
  % Både dimensions-argumenter og stabiliserings-argumenter
  The exact condition is slightly strangely worded, but if there is
  such an $i$, then the first and last columns of the matrix
  \[ (e_{\sigma(i)},\dots,e_m,a_1,\dots,a_n,\sigma_1,\dots,\sigma_i) \]
  are linearly dependent, since
  \[ \sigma_i = e_{\sigma(i)} \]
  by definition. By counting the three different groups of vectors, we
  see that the matrix has
  \[ m - \sigma(i) + 1 + n + i \]
  columns. The condition on $i$ shows that
  \[ m - \sigma(i) + 1 + n + i \leq m - \sigma(i) + \sigma(i) = m, \]
  so the conditions for being in $Y_{m,n}(\sigma)$ can never be
  satisfied, no matter the choice of vectors $a_1,\dots,a_n$. This
  shows one direction.

  The other direction
  will proceed by a purely dimensional argument. Consider the two
  flags of interest,
  \begin{alignat*}{6}
    \mathrm{Fl_R}(\Id) &&\;= (&V_1 &&\subset V_2 &&\subset \dots
    &&\subset V_m&&), \\
    \mathrm{Fl_L}(\sigma) &&\;= (&W_1 &&\subset W_2 &&\subset \dots &&\subset
                            W_m&&).
  \end{alignat*}

  Assume that the condition on $\sigma$ and $n$ is satisfied, so for
  all $i \leq m$:
  \[ 1 + n + i > \sigma(i) \]
  In the flag formulation, this means that the subspaces $V_{m-n-k}$
  and $W_k$ intersect trivially for all $k$.

  We must now find the vectors $a_1,\dots,a_n$ and will do so from
  left to right. By looking at the matrix
  \[ \left( e_1, \dots, e_m, a_1, \dots, a_n, \sigma_1, \dots,
    \sigma_m \right), \]
  we see that we must choose $a_1$ so it is not in any of the
  following subspaces of $\C^m$:
  \begin{align*}
    a_1 &\not\in V_{m-1} \\
    a_1 &\not\in V_{m-n-1} \oplus W_1 \\
    \vdots & \\
    a_1 &\not\in V_1\oplus W_{m-n-1} \\
    a_1 &\not\in W_{m-n}
  \end{align*}
  But that means choosing $a_1$ in $\C^m$ after removing a
  finite number of proper subspaces, since the dimensions of the
  spaces on the right are $m-1$ for $V_{m-1}$ and $m-n$ for
  all the rest. This space is non-empty since $n\geq 1$, so this can
  always be done. We can continue this process inductively and get an
  element $(a_1,\dots,a_n)$ of $Y_{m,n}(\sigma)$, showing that the
  space is non-empty and concluding the proof.
\end{proof}
\begin{remark}
  Note that if $n \geq m-1$, then for any $i\geq 1$ we have
  \[ 1 + n + i \geq m - 1 + 1 +i \geq m+i > m \geq \sigma(i), \] 
  and hence the space $Y_{m,n}(\sigma)$ is non-empty. So the spaces
  become non-empty as
  soon as there are ``enough'' columns.
\end{remark}
It will occasionally be nice to have an explicit element of
$Y_{m,n}(\sigma)$, but to do this we will need a slight amount of
setup. Hence it will be delayed until Theorem
\ref{thm:eksplicit-element} in Section \ref{sec:rum-stabil}.

\section{Symmetries}

The space $Y_{m,n}(\sigma)$ has quite a few symmetries that could be
of interest.
Since we are interested in linear independence of vectors in $\C^m$,
the group of complex units, $\T=(\C)^*$, acts on the space 
in several ways. The 
most important for our purposes is the following:
\begin{definition}
  For any $i \in \set{1,\dots,n}$, the group $\T$ acts
  \fixme{husk at skrive $\T = \C^*$ i en intro et sted}
  on $Y_{m,n}(\sigma)$ by scaling the $i$'th column vector,
  using the formula
  \[ \l \cdot (a_1,\dots,a_{i-1},a_i,a_{i+1},\dots,a_n) =
  (a_1,\dots,a_{i-1},\l a_i,a_{i+1},\dots,a_n). \]
  Since the determinant of a matrix is linear in each column, the
  linear independence of the vectors is preserved.

  All of these actions commute with each other, so we get an action of
  $\T^n$ on $Y_{m,n}(\sigma)$:
  \[ (\l_1,\dots,\l_n) \cdot (a_1,\dots,a_n) = (\l_1 a_1,\dots, \l_n
  a_n). \]
\end{definition}

While all of these symmetries on an open subset of $\C^{mn}$ are nice
to work with, the spaces as defined above have some issues. In
particular, as we shall see in Theorem \ref{thm:forste}, the
fundamental group and the cohomology ring of
$Y_{m,n}(\sigma)$ both become larger as $n$ grows. This complicates
some of our arguments, but luckily it can be helped by taking the
quotient with the above group action. This gives us the spaces we are
actually interested in.

\begin{definition}
  For natural numbers $n$ and $m$ and invertible matrices $X$ and $Y$,
  we define the space $X_{m,n}(X,Y)$ as the quotient of $Y_{m,n}(X,Y)$
  using the group action above,
  \[ X_{m,n}(X,Y) = Y_{m,n}(X,Y) / \T^n. \]
\end{definition}

Note that since matrix multiplication is linear, it commutes with the
action of $\T^n$ on $Y_{m,n}(X,Y)$ and we get directly from Lemma
\ref{lem:rum-perm} that for each choice of $X,Y\in \GL_m(\C)$ there is
a permutation $\sigma \in S_m$ such that
\[ X_{m,n}(X,Y) \cong X_{m,n}(\sigma). \]
Hence most of our attention will be focused on these particular
spaces. The quotient spaces are naturally closely related to the
spaces $Y_{m,n}(X,Y)$. In particular, we have the following:

\begin{lemma}
  \label{lem:reduktion}
  The spaces $Y_{m,n}(X,Y)$ and $X_{m,n}(X,Y) \times \T^n$ are homeomorphic.
\end{lemma}
\begin{proof}
  Define a map
  \[ X_{m,n}(X,Y) \to Y_{m,n}(X,Y) \]
  by mapping $A\cdot \T^n$ to the unique representative
  $\widetilde{A} \in Y_{m,n}(X,Y)$ with the last
  $n$ determinants all equal to one,
  \[ \Det_{X,Y}(\widetilde{A}) \in (\C^*)^{m-1} \times
  \{1\}^n. \]
  This map is continuous, as the composition with the quotient map,
  \[ \xymatrix{Y_{m,n}(X,Y) \ar[r]^q & X_{m,n}(X,Y) \ar[r] &
    Y_{m,n}(X,Y)}, \]
  is the map that uses the group action of $\T^n$ to change the last
  $n$ coordinates of $\Det_{X,Y}(A)$ to 1 and this is a continuous
  map. More precisely, it is given by the formula
  \[ (a_1,\dots,a_n) \mapsto \left(a_1,\dots,a_{m-1},\frac{1}{d_m}a_m,
    \frac{d_m}{d_{m+1}} a_{m+1},\dots,\hat{d}a_n\right), \]
  where $\Det_{X,Y}(a_1,\dots,a_n) = (d_1,\dots,d_{m+n-1})$ and the
  number $\hat{d}$ is an expression consisting of products and
  quotients of the last $n$ of these
  determinants, the exact terms of which depends on $n$ and $m$. If
  $\Det_{X,Y}(A) = (d_1,\dots,d_{m+n-1})$ denotes the determinants as
  above, the homeomorphism is the map
  \begin{align*}
    Y_{m,n}(X,Y) &\to X_{m,n}(X,Y) \times \T^n\\
    A &\mapsto (A\cdot \T^n, (d_m,\dots,d_{m+n-1})),
  \end{align*}
  with inverse
  \[ (A\cdot \T^n, (d_m,\dots, d_{m+n-1})) \mapsto
  \left(\overbrace{1,\dots,1}^{m-1},d_m,\frac{d_{m+1}}{d_m},\dots,
    \frac{1}{\hat{d}}\right) \cdot f(A). \]
\end{proof}

The above lemma is useful, since it oftens allows us to work with the
unquotiented spaces and still gain information about the
quotients. Note that in the definition of the homeomorphism
\[ Y_{m,n}(X,Y) \cong X_{m,n}(X,Y) \times \T^n , \]
we might as well have
used the first $n$ determinants instead of the last, and we will
occasionally do this if it is easier for the arguments. \fixme{Tjek
  lige om vi faktisk gør dette, ellers så fjern det}
This lemma also immediately proves the claim that the fundamental
group and cohomology ring of $Y_{m,n}(X,Y)$ grows with $n$, by the
formulas
\begin{align*}
  \pi_1(Y_{m,n}(X,Y)) &\cong \pi_1(X_{m,n}(X,Y)) \times \Z^n, \\
  H^*(Y_{m,n}(X,Y)) &\cong H^*(X_{m,n}(X,Y)) \otimes H^*(\T^n).
\end{align*}
But more importantly, the last formula allows us to calculate the
cohomology of $X_{m,n}(X,Y)$ from the cohomology of $Y_{m,n}(X,Y)$, by
quotienting out the free groups coming from the cohomology ring of the
torus $H^*(\T^n)$. For example, we will later compute 
\[ H^1(X_{m,n}) \cong \Z^{m-1} \]
by instead computing
\[ H^1(X_{m,n}) \oplus \Z^n \cong H^1(Y_{m,n}) \cong \Z^{m+n-1}. \]

% Stabilisering
%% Flere retninger? Vi bruger ikke rigtig X_{m,n} -> X_{m+1,n}
%% Dette kunne måske linkes til Bruhat (så en introduktion til dette?)
%% Og definer grænsen, X_{m,\infty}
\section{Stabilisation} 
\label{sec:rum-stabil}

We would like to relate the spaces $Y_{m,n}(\sigma)$ that we obtain
for different values of
$m$ and $n$. Since an invertible, lower triangular matix $L$ preserves
the flag $\mathrm{Fl}_R(\Id)$, we will try to use this to define a map
\begin{align*}
  s : Y_{m,n} &\to Y_{m,n+1} \\
  s(a_1,\dots,a_{n}) &= (La_1,\dots,La_{n},Le_1).
\end{align*}
At first, consider the spaces $Y_{m,n}$ and $Y_{m,n+1}$.
To ensure the right hand side actually is an element of $Y_{m,n+1}$,
we will start by considering the inverse of $s$, mapping a subset of
$Y_{m,n+1}$ to $Y_{m,n}$.
Consider an element $A=(a_1,\dots,a_{n+1})$ in $Y_{m,n+1}$ where
the entries of $a_{n+1}$ are all equal to one:
\[ A = 
\begin{pmatrix}
  \vrule & \vrule & & \vrule & \vspace{-0.5em}
  1 \\
  a_1 & a_2 & \dots & a_{n} & \vdots \\
  \vrule & \vrule & & \vrule & 1
\end{pmatrix}. \] 
Then we can multiply through with the lower triangular, invertible
matrix $L^{-1}$ with ones on the diagonal, negative ones just below
the diagonal and zeroes everywhere else,
\[ L^{-1} =
\begin{pmatrix}
   1 &  0 & 0 & \dots &  0 & 0 \\
  -1 &  1 & 0 & \dots &  0 & 0 \\
   0 & -1 & 1 & \dots &  0 & 0 \\
   \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
   0 &  0 & 0 & \dots &  1 & 0 \\
   0 &  0 & 0 & \dots & -1 & 1
\end{pmatrix}. \]
This reduces $A$ to
\[ L^{-1}A =
\begin{pmatrix}
  \vrule & \vrule & & \vrule & 
  1 \\
  \vrule & \vrule & & \vrule & \vspace{-0.5em}
  0 \\
  L^{-1}a_1 & L^{-1}a_2 & \dots & L^{-1}a_{n} & \vdots \\
  \vrule & \vrule & & \vrule & 0 \\
  \vrule & \vrule & & \vrule & 0
\end{pmatrix}, \]
and going a bit further we can see that
$(L^{-1}a_1,\dots,L^{-1}a_{n}) \in Y_{m,n}$ by computing
\[ L^{-1}\cdot \left(\Id_m,a_1,\dots,a_{n+1},\Id_m\right) =
\left(L^{-1},L^{-1}a_1,\dots,L^{-1}a_{n},
  \begin{array}{cccccc}
    1 & 1 & 0 & \dots & \dots & 0 \\
    0 & -1 & 1 & \dots & \dots& 0\\
    0 & 0 & -1 & \ddots & & \vdots\\
    \vdots & \vdots & & \ddots & 1 & 0 \\
    0 & 0 & \dots & & -1& 1
  \end{array}
\right). \] \fixme{Få til at se bedre ud og fylde mindre. hbox error}
This shows that $L^{-1}\cdot(a_1,\dots,a_{n})$ is in
$Y_{m,n}(L^{-1},U)$, where $U$ is the
upper triangular matrix given above. But this space is the same as
$Y_{m,n}$ since the right flag of a lower triangular matrix and the
left flag of an upper triangular matrix
is the same as the corresponding flag of the identity
matrix, as we saw in Section \ref{sec:flags}.

Since the above is done by multiplying with a matrix, it commutes with
the action of the torus $\T^n$ on $Y_{m,n}$, and everything can be
repeated with $Y_{m,n}$ and $Y_{m,n+1}$ replaced by the spaces
$X_{m,n}$ and $X_{m,n+1}$.

The above considerations allow us define a map that relates the spaces
$X_{m,n}$ and $X_{m,n+1}$, by working with the inverse matrix $L$.
\begin{definition}
  \label{def:stabilisering}
  The \textit{stabilisation map} $s : X_{m,n} \to X_{m,n+1}$ is given
  by
  \[ s\left(a_1,\dots,a_{n-1}\right) =
  \left(La_1,\dots,La_{n-1},Le_1\right), \]
  where $L$ is the lower triangular matrix with every entry below the
  diagonal equal to 1,
  \[ L =
  \begin{pmatrix}
    1 & 0 & 0 & \dots & 0 \\
    1 & 1 & 0 & \dots & 0 \\
    1 & 1 & 1 & \dots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & 1 & 1 & \dots & 1
  \end{pmatrix}. \]
\end{definition}

In the above, we only considered the spaces $X_{m,n}$ and $X_{m,n+1}$
defined by the identity permutation $\Id\in S_m$,
but there is a similar stabilisation for the other permutations. We
will again start by considering the unreduced spaces
$Y_{m,n}(\sigma)$. For
any permutation $\sigma$, the space $Y_{m,n+1}(\sigma)$ is a union of
spaces homeomorphic to a torus times $Y_{m,n}(\tau)$, for various
permutations $\tau\in S_m$. The exact relation is given by the
following lemma.

\begin{lemma}
  \label{lem:permutation}
  Any choice of indices $I= (i_1<\dots<i_k) \subset \set{1,\dots,m}$
  defines a subspace of $Y_{m,n+1}(\sigma)$:
  \[ Y_{m,n+1}^I(\sigma) = \set{(a_1,\dots,a_{n+1}) \in
    Y_{m,n+1}(\sigma) \sdel 
    \begin{matrix} 
      (a_{n+1})_{i_j} \neq 0\; \forall i_j \in I, \\
      (a_{n+1})_j = 0 \;\forall j \not\in I
    \end{matrix} }. \]
  Here $(a_{n+1})_j$ denotes the $j$'th entry of the vector $a_{n+1}$.
  This space is empty if $I$ does not contain the number $\sigma(m)$,
  so we will assume that $\sigma(m) \in I$ but otherwise leave it out
  of the notation.
  
  These subspaces are disjoint and cover $Y_{m,n+1}(\sigma)$. There is
  a map $\varphi$,
  defined for an indexing set $I$ and a permutation $\sigma$, which
  gives a new permutation such that
  \[ Y_{m,n+1}^I(\sigma) \cong Y_{m,n}(\varphi(I,\sigma)) \times
  (\C^*)^{\size{I}}. \]
  If we define the permutation $\widehat \sigma = \sigma \cdot
  \big(m\; m-1\; \dots \; 1\big)$ and the indexing set is ${I = (i_1 <
    \dots < i_k)}$, then $\varphi$ is given by
  \[ \varphi\big(I,\sigma\big) = 
  \begin{cases}
    \begin{matrix} \widehat\sigma \end{matrix} & 
    \begin{array}{l} k = 0 \end{array} \\
    \begin{matrix}
      \big( i_k\; \sigma(m) \big) \cdot \varphi\left( (i_1 < \dots <
        i_{k-1}),\sigma\right) \\
    \end{matrix} &
    \begin{array}{l} i_k < \sigma(m) \text{ and } \\
      \widehat\sigma^{-1}(i_k) > \widehat\sigma^{-1}(i_r) \,
      \forall r < k\end{array}\\
    \begin{matrix}
      \varphi\left( (i_1 < \dots < i_{k-1}),\sigma\right)
    \end{matrix} &
    \begin{array}{l} \text{otherwise} \end{array}
  \end{cases}. \]
  Note that the index $\sigma(m)$ has once again been removed from the
  notation, 
  so the $k = 0$ case is really $I = (\sigma(m))$ and $i_k$
  is the last index not equal to $\sigma(m)$.
\end{lemma} \fixme{måske skal beviset skrives om. Det er muligvis
  rodet/for langt}
\begin{proof}
  Fix the permutation $\sigma \in S_m$. The observation that
  $Y^I_{m,n+1}(\sigma)$ is empty 
  if $\sigma(m)$ is not in $I$ follows from considering the matrix
  \[ (a_{n+1},\sigma_1,\dots,\sigma_{m-1}) =
  (a_{n+1},e_{\sigma(1)},\dots,e_{\sigma(m-1)}). \]
  For this matrix to be invertible we must have $(a_{n+1})_{\sigma(m)}
  \neq 0$. If $\sigma(m)$ is not in $I$, then the conditions on
  $Y^{I}_{m,n+1}(\sigma)$ can never be satisfied and the space must be
  empty. We will from now on assume that $I$ contains $\sigma(m)$.
  
  To see that the spaces are disjoint and cover all of
  $Y_{m,n+1}(\sigma)$, we can see that an element
  $(a_1,\dots,a_{n+1})\in Y_{m,n+1}(\sigma)$ is in exactly one
  subspace, namely the one corresponding to $I = \set{ j \in
    \set{1,\dots,m} \mid (a_{n+1})_j \neq 0}$.
  
  To define the function $\varphi$ and the homeomorphism
  \[ Y^I_{m,n+1}(\sigma) \cong Y_{m,n}(\varphi(I,\sigma))\times
  (\C^*)^{\size{I}}, \]
  we will proceed as in the previous case. The map to the torus
  $(\C^*)^{\size{I}}$ is given by remembering the non-zero entries of
  $a_{n+1}$, so we can immediately reduce to the case where all
  entries of $a_{n+1}$ are either zero or one.

  An element of $Y_{m,n+1}^I(\sigma)$ consists of vectors
  $(a_1,\dots,a_{n+1})$ with the matrix
  \[ M = (a_{n+1},\sigma_1,\dots,\sigma_{m-1}) \]
  invertible. It is this matrix that we want to factor as
  $L\cdot \varphi(I,\sigma)\cdot U$, where $L$ is an invertible, lower
  triangular matrix and $U$ is an invertible, upper triangular
  matrix. To do this, we want to make row operations on the matrix
  corresponding to multiplying with $L^{-1}$ until we end up with an
  upper triangular matrix with permuted rows, corresponding to
  $\varphi(I,\sigma) \cdot U$. Multiplying with $L^{-1}$ corresponds
  to making row operations where each row is only allowed to affect
  the rows below it. We want to remove all non-zero entries in the
  first column except for one, and we want this single entry to have
  the lowest possible index. We also want to make the matrix as close
  as possible to upper triangular, which is done by making sure that
  each operation does not introduce a non-zero entry to the left of an
  already existing non-zero entry.
  
  If $k = 0$, an element $Y^I_{m,n+1}(\sigma)$ looks like
  \[ (a_1,\dots,a_n,\sigma_m). \]
  Since we are in $Y_{m,n+1}(\sigma)$, the linear independence
  condition gives us that
  \[ (a_1,\dots,a_n) \in Y_{m,n}(\tau), \]
  where $\tau$ is the permutation
  \[ \tau(k) =
  \begin{cases}
    \sigma(k-1) & k > 1 \\
    \sigma(m) & k = 1
  \end{cases}. \]
  We recognize this as the permutation $\widehat\sigma$ and get the
  formula
  \[ \varphi((\sigma(m)),\sigma) = \widehat\sigma. \]
  The homeomorphism is in this case given by
  \begin{align*}
    Y_{m,n+1}^{(\sigma(m))}(\sigma) &\to Y_{m,n}(\widehat\sigma)\times
                                      \C^* \\
    (a_1,\dots,a_n,\lambda\cdot\sigma_m) &\mapsto
                                           (a_1,\dots,a_n,\lambda)
  \end{align*}
  
  Now consider the case $k > 0$, so $I = (i_1 < \dots < i_k)$. If $i_k
  > \sigma(m)$, we can write
  $(a_{n+1},\sigma_1,\dots,\sigma_{m-1})$ in block form as
  \[
  \begin{pmatrix}
    I_1 & A \\
    1 & 0 \\
    I_2 & B \\
    1 & C \\
    0 & D
  \end{pmatrix}.
  \]
  $I_1$ is a vector with ones on the entries corresponding
  to $\set{i_j \mid i_j < \sigma(m)}$ and zeroes elsewhere, while
  $I_2$ is similar except we consider indices greater than
  $\sigma(m)$. The first 1 is on the $\sigma(m)$'th row and the next
  is on the $i_k$'th row. All entries below are zero. $A$, $B$, $C$
  and $D$ are the matrices consisting of the corresponding rows from
  $(\sigma_1,\dots,\sigma_{m-1})$. But by multiplying with a lower
  triangular matrix, we can reduce this matrix to
  \[
  \begin{pmatrix}
    I_1 & A \\
    1 & 0 \\
    I_2 & B \\
    0 & C \\
    0 & D
  \end{pmatrix}.
  \]
  But this shows that $\varphi(I,\sigma) =
  \varphi\big((i_1<\dots<i_{k-1}),\sigma\big)$ when $i_k >
  \sigma(m)$.

  By possibly using the above multiple times, we reduce to the case
  where $i_k < \sigma(m)$. Now we would like to reduce the length of
  $I$ by one, and consider how this affects the resulting permutation.
  Consider $\widehat\sigma^{-1}(i_k)$. If
  there is a $j < k$ with $\widehat\sigma^{-1}(i_k) <
  \widehat\sigma^{-1}(i_j)$, then we can write
  $(a_{n+1},\sigma_1,\dots,\sigma_{m-1})$ as
  \[
  \begin{pmatrix}
    \vdots & \\
    1 & \dots & \dots & \dots & 1 & \dots\\
    \vdots \\
    1 & \dots & 1 & \dots & \dots & \dots\\
    \vdots & \\
    1 &  \dots & \dots & \dots & \dots & \dots\\
    \vdots
  \end{pmatrix}. \]
  In the above, only the $i_j$'th, $i_k$'th and $\sigma(m)$'th rows
  have been written out, and only the non-zero entries. But this
  matrix can be reduced to
  \[
  \begin{pmatrix}
    \vdots & \\
    1 & \dots & \dots & \dots & 1 & \dots\\
    \vdots \\
    0 & \dots & 1 & \dots & -1 & \dots\\
    \vdots & \\
    1 & \dots & \dots  & \dots & \dots & \dots\\
    \vdots
  \end{pmatrix}. \]
  We are now unable to change row $i_k$, since there are no non-zero
  entries above the left-most one. The additional $-1$ that was
  introduced can be removed by multiplying on the right with an upper
  triangular matrix that subtracts column $\widehat\sigma^{-1}(i_k)$
  from column $\widehat\sigma^{-1}(i_j)$. So we are able to remove the
  one in the $i_k$'th row without changing the resulting permutation,
  showing that $\varphi\big(I,\sigma\big) =
  \varphi\big((i_1<\dots<i_{k-1}),\sigma\big)$ in this case.

  Now assume that there is no such $j$. Then the row $i_k$ is our best
  candidate for removing the bottom-most one in row $\sigma(m)$. If we
  do this row operation, by multiplying with $\widehat L$, we get
  \[ \varphi(I,\sigma)\cdot U = L^{-1}\widehat{L} \cdot M \]
  But when we perform row operations on $\widehat{L} \cdot M$, we are
  only changing rows with index less than or equal to $i_{k-1}$, since
  this is the last non-zero entry in the first column. Hence $L^{-1}$
  has the form
  \[ L^{-1} = 
  \begin{pmatrix}
    \vdots \\
    * & \dots & * & 1 & \dots & \dots & \dots \\
    \vdots \\
    0 & \dots & \dots & \dots & \dots & 1 & \dots \\
    \vdots
  \end{pmatrix},
  % \left(
  %   \begin{array}{c|c}
        %         L' & 0 \\
        %         \hline
        %         0 & \Id
                      %       \end{array} \right)
  \]
  where we once again write out row $i_k$ and $\sigma(m)$, and use the
  stars to indicate entries that may be non-zero but that are not
  important.
  If we switch row $i_k$ and row $\sigma(m)$ along with column
  $i_k$ and column $\sigma(m)$, we get another lower triangular
  matrix:\fixme{beslut (overalt) om der bruges , eller ; i
    permutationer}
  \[ (i_k \; \sigma(m)) L^{-1} (i_k \; \sigma(m)) = 
  \begin{pmatrix}
    \vdots \\
    0 & \dots & \dots & 1 & \dots & \dots & \dots \\
    \vdots \\
    * & \dots & * & \dots & \dots & 1 & \dots \\
    \vdots
  \end{pmatrix}.\]
  Inserting in our original equation, we see
  \[ (i_k \; \sigma(m)) \varphi(I,\sigma) \cdot U = L^{-1} \cdot (i_k \;
  \sigma(m)) \cdot \widehat{L} M. \]
  But $(i_k \; \sigma(m)) \cdot \widehat{L} M$ is just $M$ with the
  one in the first column, row $i_k$, replaced by zero. This gives the
  formula
  \[ (i_k \; \sigma(m)) \cdot \varphi(I,\sigma) =
  \varphi\big((i_1<\dots<i_{k-1}),\sigma\big) \]
  and our desired formula for $\varphi$ follows by multiplying over.
\end{proof}
\fixme{Sørg for at permutations-notationen er beskrevet,
  e.g. $\sigma_1 = e_{\sigma(1)}$.}

By looking at the formula for $\varphi$, we can deduce the following
corollary that shows how the different possible permutations that can
show up are related.

\begin{corollary}
  Blah blah, $\varphi(J,\sigma) \leq \varphi(I,\sigma)$ for $I \subset
  J$. \fixme{Vis det her. Slå op i noter}
\end{corollary}
\begin{proof}
  
\end{proof}

With the stabilisation map, we can now give explicit elements of
$Y_{m,n}(\sigma)$, as was hinted at earlier.

\begin{theorem}
  \label{thm:eksplicit-element}
  If the space $Y_{m,n}(\sigma)$ is non-empty, then the sequence of
  vectors
  \[ s^n(e_1) = \left(L^n e_1,\dots,Le_1\right) \]
  belongs to $Y_{m,n}(\sigma)$.
\end{theorem}
\begin{proof}
  For a discussion of then $Y_{m,n}(\sigma)$ is non-empty, consult
  Theorem \ref{thm:ikke-tom}.
  
  We already know that the sequence
  \[ \left(e_1,\dots,e_m,L^n e_1,\dots, Le_1 \right) \]
  has all subsequent sequences of $m$ vectors linearly independent,
  by the construction of the stabilisation map. So the thing to check
  is whether the matrices
  \[ \left(e_{k+n+1}, \dots, e_m,L^n e_1, \dots, Le_1, \sigma_1, \dots,
    \sigma_{k}\right) \]
  are invertible, for $k \in \set{1,\dots,m-1}$. If $n+k$ are greater
  than or equal to $m$, we of course mean the matrices
  \[ \left( L^{m-k}e_1,\dots,L e_1,\sigma_1,\dots,\sigma_k \right). \]
  
  Calculating the determinant of a matrix of this form by column
  expansion, this is equivalent to choosing $s=m-k$ numbers
  $i_1,\dots,i_s \in \set{1,\dots,m},$ with $i_j < i_{j+1},$ and checking that the matrix
  \[
  \begin{pmatrix}
    \matrow{r_{i_1}} \\
    \matrow{r_{i_2}} \\
    & \hspace{-0.75em} \vdots & \\
    \matrow{r_{i_s}}
  \end{pmatrix}
  \]
  is invertible, where $r_{l}$ denotes the $l$'th row of the
  matrix
  \[ P = (L^{m-k}e_1,\dots,Le_1). \]
  
  Calculating this matrix by induction, we see that it is related to
  Pascal's triangle,
  \[ P =
  \begin{pmatrix}
    p_{i,j}
  \end{pmatrix}
  =
  \begin{pmatrix}
    {m-k-1 \choose 0}\vspace{1em} & {m-k-2\choose 0} & \dots &
    {1\choose 0} & {0 \choose 0} \\
    {m-k \choose 1} \vspace{0.5em} & {m-k-1 \choose 1} & \dots & {2
      \choose 1} & {1 \choose 1} \\
    \vdots \vspace{0.5em} & \vdots & \ddots & \vdots & \vdots\\
    {2(m-k-1) \choose {m-k-1}} & {2(m-k-1)-1 \choose m-k-1} & \dots &
    {m-k \choose m-k-1} & {m-k-1 \choose m-k-1}
  \end{pmatrix}, \]
  with the entry in the $i$'th row and $j$'th column given by the
  formula
  \[ p_{i,j} = { m-k-j+i-1 \choose i-1}. \] \fixme{Skriv beviset for
    dette ud}
  
  In particular, by the standard formulas for binomial coefficients,
  \[ p_{i,j} = p_{i-1,j} + p_{i,j+1}, \]
  so the row $r_{j}$ can be calculated by taking the row
  above,
  $r_{j-1}$, and adding the row $\widehat{r}_j$ given by shifting
  every entry of $r_j$ one to the left and adding a zero,
  \begin{equation}
    \label{eq:rows}
    r_j = r_{j-1} + \widehat{r}_j.
  \end{equation}
  To illustrate, if $m-k$ is three, then $P$ is the matrix
  \[ P = 
  \begin{pmatrix}
    1 &1 & 1 \\
    3 &2 & 1 \\
    6 & 3 & 1
  \end{pmatrix} \]
  and the third row is given by
  \[
  \begin{pmatrix}
    6 &3 & 1
  \end{pmatrix} = r_3 = r_2 + \widehat{r}_3 =
  \begin{pmatrix}
    3 & 2 & 1
  \end{pmatrix} +
  \begin{pmatrix}
    3 & 1 & 0
  \end{pmatrix} \]
  
  We will prove that the matrix
  \[ R = 
  \begin{pmatrix}
    \matrow{r_{i_1}} \\
    \matrow{r_{i_2}} \\
    & \hspace{-0.75em} \vdots & \\
    \matrow{r_{i_s}}
  \end{pmatrix}
  \]
  is invertible by induction on $s$. The exact statement we intend to
  prove is that the determinant of $R$ is strictly positive if two
  divides $s$ an even number of times and is strictly negative if
  two divides $s$ an odd number of times.
  For $s = 1$, $R$ is always the matrix $R = \left( 1 \right)$ and we
  are done since $\det R = 1$ is strictly positive and two divides $s$
  zero times.
  Assume the statement is correct for $s-1$. Since the determinant is
  linear in each row, we can use Equation \ref{eq:rows} and get
  \begin{align*}
    \det{R} &= 
              \det\begin{pmatrix}
                \matrow{r_{i_1}} \\
                \matrow{r_{i_2}} \\
                & \hspace{-0.75em} \vdots & \\
                \matrow{r_{i_{s-1}}} \\
                \matrow{r_{i_s}}
              \end{pmatrix} \\
            &= 
              \det\begin{pmatrix}
                \matrow{r_{i_1}} \\
                \matrow{r_{i_2}} \\
                & \hspace{-0.75em} \vdots & \\
                \matrow{r_{i_{s-1}}} \\
                \matrow{r_{i_s-1}}
              \end{pmatrix}+
    \det\begin{pmatrix}
      \matrow{r_{i_1}} \\
      \matrow{r_{i_2}} \\
      & \hspace{-0.75em} \vdots & \\
      \matrow{r_{i_{s-1}}} \\
      \matrow{\widehat{r}_{i_s}}
    \end{pmatrix} \\
            &=   \det\begin{pmatrix}
              \matrow{r_{i_1}} \\
              \matrow{r_{i_2}} \\
              & \hspace{-0.75em} \vdots & \\
              \matrow{r_{i_{s-1}}} \\
              \matrow{r_{i_{s-1}}}
            \end{pmatrix}
    + \sum_{j=0}^{i_s-i_{s-1}-1} \det\begin{pmatrix}
      \matrow{r_{i_1}} \\
      \matrow{r_{i_2}} \\
      & \hspace{-0.75em} \vdots & \\
      \matrow{r_{i_{s-1}}} \\
      \matrow{\widehat{r}_{i_s-j}} \end{pmatrix}.
  \end{align*}
  The first determinant is zero, since the matrix has two equal rows,
  so we are left with the sum. Doing the same thing for all the other
  rows except for the first one gives us
  \[ \det R = \sum_{j_1 = 0}^{i_2-i_1-1}\sum_{j_2 = 0}^{i_3-i_2-1}
  \dots \sum_{j_{s-1}=0}^{i_s-i_{s-1}-1} \det
  \begin{pmatrix}
    \matrow{r_{i_1}} \\
    \matrow{\widehat{r}_{i_2-j_1}} \\
    & \hspace{-0.75em} \vdots & \\
    \matrow{\widehat{r}_{i_{s-1}-j_{s-2}}} \\
    \matrow{\widehat{r}_{i_s-j_{s-1}}}
  \end{pmatrix}. \]

  By considering the last column of one of these matrices, we see that
  it has the form
  \[
  \begin{pmatrix}
    1 \\
    0 \\
    \vdots \\
    0
  \end{pmatrix} \]
  since we have shifted every row below the first one space to the
  left. By expanding the determinant using this column, we remove it and
  end up with
  \[ \det R = \sum_{j_1 = 0}^{i_2-i_1-1}\sum_{j_2 = 0}^{i_3-i_2-1}
  \dots \sum_{j_{s-1}=0}^{i_s-i_{s-1}-1} (-1)^{s-1} \det
  \begin{pmatrix}
    \matrow{\widetilde{r}_{i_2-j_1}} \\
    & \hspace{-0.75em} \vdots & \\
    \matrow{\widetilde{r}_{i_{s-1}-j_{s-2}}} \\
    \matrow{\widetilde{r}_{i_s-j_{s-1}}}
  \end{pmatrix}. \]
  Here we use $\widetilde{r}_i$ to denote the $i$'th row of $P$ where we
  have removed the leftmost entry. But we can now apply the induction
  hypothesis. We see that all the determinants have the same sign, are
  non-zero and
  so $\det R$ is non-zero with opposite sign if $s-1$ is odd and
  otherwise has the
  same sign. This exactly means that $\det R$ changes sign
  when $s$ passes a multiple of two, as stated above. \fixme{Tjek lige
    det argument for fortegn igennem igen...}
\end{proof}

We now have quite a large collection of spaces, and quite a few
elements in them. We would like to be able to collect them all in a
single space. \fixme{Skriv det her om eller skriv noget bedre...}

\section{The limit space}
\label{sec:rum-gr}

Using the stabilisation defined previously, we get a directed system
of topological spaces,
\[ \xymatrix{X_{m,1} \ar[r]^s & X_{m,2} \ar[r]^s & \dots \ar[r]^s &
  X_{m,n} \ar[r]^s & X_{m,n+1} \ar[r]^s & \dots } \]
When considering such a system, it can sometimes be helpful to
take the limit as $n$ approaches infinity.

\begin{definition}
  The space $X_{m,\infty}$ is defined as the direct limit of the
  directed system of spaces,
  \[ X_{m,\infty} = \varinjlim_n X_{m,n}. \]
  As a set, $X_{m,\infty}$ is the disjoint union of all the spaces,
  where we identify $A \in X_{m,k}$ with $B \in X_{m,k'}$ if they
  eventually map to the same thing under $s$:
  \[ A \sim B \iff \exists\, i,j : s^i(A) = s^j(B). \]
  We give $X_{m,\infty}$ the finest topology such that all the
  maps 
  \[ \imath_n : X_{m,n} \to X_{m,\infty} \]
  are continuous. So a subset $U \subset X_{m,\infty}$ is open if and
  only if $\imath_n^{-1}(U)$ is open in $X_{m,n}$ for all n. \fixme{Tjek
    continuous alle steder...}
\end{definition}

We could just as well define the space \fixme{Overvej lige om det her
  er rigtigt og om det bare skal fjernes igen}
\[ Y_{m,\infty} = \varinjlim_n Y_{m,n} \]
and note that all stabilisation maps are linear. This means we get an
action on $Y_{m,\infty}$ of the infinite torus,
\[ \T^\infty = \varinjlim_n \T^n, \]
defined by taking the
limit of the
inclusion maps $\T^n \to \T^{n+1}$ that inserts a one on the last
coordinate.
Then we could define the limit space as
\[ X_{m,\infty} = Y_{m,\infty}/\T^\infty. \]

% Noget med at $s^m \simeq \Id$?

\fixme{Andre koefficienter her?}

% Hvis ikke introduceret endnu ville spektralfølgen give mening
% her. Hvis allerede introduceret i det generelle tilfælde ville det
% nok give mening at specialisere til Y_{m,n} eller X_{m,n} her og
% beskrive de forskellige søjler.
%% Her ville det måske også give mening at se på H^1 og vise at den
%% giver det "rigtige" svar, \Z^{m-1}, for at vise at følgen kan
%% bruges til noget.

\fixme{Flyt dette hen hvor det giver mening. Muligvis et andet
  kapitel.}

\section{The first cohomology of $X_{m,n}$}
\label{sec:firstcohom}

We would like to use the spectral sequence to compute the first
cohomology group of $X_{m,n}$ for any $m$ and $n$. As discussed at the
end of Section \ref{sec:rum}, we will do this by computing
$H^1(Y_{m,n})$ and
then using Lemma \ref{lem:reduktion} to calculate the cohomology of
$X_{m,n}$ via factoring out the extra free group from the torus
in the isomorphism
\[ H^1(Y_{m,n}) \cong H^1(X_{m,n}) \oplus H^1(\T^n). \]

From the theory
underlying the spectral sequence, see e.g. \cite[Chapter
1]{hatcherss}, we get that there is a subgroup
\[ 0 \subset H \subset H^1(Y_{m,n}) \]
and isomorphisms
\begin{align*}
  E_\infty^{0,1} &\cong H^1(Y_{m,n})/H, \\
  E_\infty^{1,0} &\cong H.
\end{align*}
So to calculate $H^1(Y_{m,n})$, we have to calculate the two groups
$E_\infty^{1,0}$ and $E_\infty^{0,1}$ and see what groups fit
into the short exact sequence
\[ 0 \to E^{1,0}_\infty \to H^1(Y_{m,n}) \to E^{0,1}_\infty \to 0. \]

Drawing the relevant part of the first page of the spectral sequence,
we are considering the diagram illustrated in Figure
\ref{fig:foerste}.
\begin{figure}[ht]
  \[ \xy
  (5,4); (5,-13); **\dir{-}; (5,4); *\dir{>};
  (5,-13); (73,-13); **\dir{-}; *\dir{<};
  (0,-15)*{q}; (5,-17)*{p};
  (1,-17); (4,-14); **\dir{-};
  (0,0)\xymatrix@R=1em@C=1.7em{
    1 & H^1(F_0) \ar[r]& H^2(F_1,F_0) \ar[r] & H^3(F_2,F_1) \\
    0 & H^0(F_0) \ar[r] & H^1(F_1,F_0) \ar[r] & H^2(F_2,F_1) \\
    & 0 & 1 & 2
  } \endxy \]
  \caption{The relevant part of $E_1$ for computing $H^1(Y_{m,n})$.}
  \label{fig:foerste}
\end{figure}
But by writing out our specific case, we can see that the the groups
$H^1(F_1,F_0), H^2(F_2,F_1)$ and $H^3(F_2,F_1)$ are all zero, so the
diagram simplifies to Figure \ref{fig:anden}. \fixme{Gør dette
  tidligere}
\begin{figure}[ht]
  \[ \xy
  (5,4); (5,-13); **\dir{-}; (5,4); *\dir{>};
  (5,-13); (75,-13); **\dir{-}; *\dir{<};
  (0,-15)*{q}; (5,-17)*{p};
  (1,-17); (4,-14); **\dir{-};
  (0,0)\xymatrix@R=1em@C=1.7em{
    1 & H^1(Y_{m,n-1}\times \T^m) \ar[r]& H^2(F_1,F_0) & 0 \\
    0 & H^0(Y_{m,n-1}\times \T^{m}) & 0 & 0 \\
    & 0 & 1 & 2
  } \endxy \]
  \caption{The simplified $E_1$.}
  \label{fig:anden}
\end{figure}
From the figure, we can see that the group $E_\infty^{1,0}$
must be zero, so the exact sequence we are considering gives an
isomorphism
\[ \xymatrix{0 \ar[r] & H^1(Y_{m,n}) \ar[r]^>>>>>{\cong} & E^{0,1}_\infty
  \ar[r] & 0}. \]

By doing explicit calculations for small $m$ and $n$, we are led to
the following theorem. \fixme{Inkluder disse/henvis til eksemplerne i
  starten}

\begin{theorem}
  \label{thm:forste}
  The first cohomology group of $Y_{m,n}$ is isomorphic to $\Z^{m+n-1}$.
\end{theorem}
\begin{proof}
  We will proceed by induction on $n$. By looking at Example
  \ref{ex:n=1}, we see that
  \[ H^1(Y_{m,1}) \cong H^1(\T^{m}) \cong \Z^m = \Z^{m+1-1} \]
  as desired.

  Now assume the theorem is correct for 
  \[ H^1(Y_{m,n-1}) \cong \Z^{m+n-2}. \] \fixme{Bør vise at $Y_{m,n}$
    0-sh tidligere}
  By excision, we know that the group $H^2(F_1,F_0)$ is given by
  \[ H^2(F_1,F_0) \cong \bigoplus_{i=1}^{m-1} H^0(Y_{m,n-1}\big((i\;
  i+1)\big) \otimes H^2(D,D-0) \cong \Z^{m-1}. \]\fixme{Sørg for
    at vi ved dette}
  \begin{figure}[ht]
    \[ \xy
    (5,4); (5,-13); **\dir{-}; (5,4); *\dir{>};
    (5,-13); (61,-13); **\dir{-}; *\dir{<};
    (0,-15)*{q}; (5,-17)*{p};
    (1,-17); (4,-14); **\dir{-};
    (0,0)\xymatrix@R=1em@C=1.7em{
      1 & \Z^{m+n-2}\oplus\Z^m \ar[r]& \Z^{m-1} & 0 \\
      0 & \Z & 0 & 0 \\
      & 0 & 1 & 2
    } \endxy \]
    \caption{$E_1$, assuming the induction hypothesis.}
    \label{fig:tredje}
  \end{figure}
  This simplifies Figure \ref{fig:anden} to Figure
  \ref{fig:tredje}. Since the groups involved will never change after
  turning the page to $E_2$, we see that we only have to compute the
  group
  \[ E_\infty^{0,1} = E_2^{0,1} = \ker \left( d_1 : E_1^{0,1} \to
    E_1^{1,1} \right). \]
  The differential $d_1$ is given by the differential in the long
  exact sequence for the pair $(F_1,F_0)$. By studying the long exact
  sequence, the differential is given by the composition
  \fixme{Beskriv tidligere?}
  \[ d_1 : \xymatrix{ H^1\left(Y_{m,n-1}\times \T^m\right)
    \ar[r]^<<<<<<<<<{\oplus\imath^*}  & \bigoplus_{i=1}^{m-1}
    H^1\left(Y_{m,n-1}\big((i\; i+1)\big)\times \T^{m-1} \times
      (D-0)\right)  \\
    \hspace{5em}\ar[r]^<<<<<{\oplus\delta} & \bigoplus_{i=1}^{m-1}
    H^2\left(Y_{m,n}\big((i\; i+1)\big)\times \T^{m-1} \times
      (D,D-0)\right).}\] \fixme{Få pilen til at sidde ordentligt}
  The first map is induced by the inclusions
  \[ \imath : Y_{m,n-1}\big((i\; i+1)\big) \times \T^{m-1} \times (D-0)
  \to Y^{m,n-1}\times \T^m, \] 
  given by first using the inverse of the homeomorphism from Lemma
  \ref{lem:permutation}, given by the indices $I = (1 < 2 < \dots <
  i-1 < i+1 < \dots m)$,
  \[ Y_{m,n}^I \to Y_{m,n+1}\big((i \; i+1)\big) \times \T^{m-1}, \]
  and then inserting the coordinate from $(D-0)$ on the $i$'th
  coordinate of the last column. \fixme{Beskriv at $D-0$ er meget
    lille et sted tidligere}
  The second map consists of the differentials from the long exact
  sequences of the pairs
  \[ Y_{m,n-1}\big((i\; i+1)\big) \times \T^{m-1}\times (D,D-0). \]
  We want to see that $d_1$ is surjective, so we will compute the
  image of the element 
  \[ \alpha_j = 1\otimes\dots 1\otimes [T] \otimes 1 \otimes \dots
  \otimes 1 \in H^1(Y_{m,n-1}\times \T^{m}),\] 
  where the class $[T]\in H^1(T)$ is in the $j$'th tensor coordinate.
  We will consider each inclusion separately. If $i \neq j$, the
  inclusion will not touch the $j$'th coordinate of $\T^m$, so on
  cohomology classes it will map $\alpha_j$ to an element that is of
  the form
  \[ \imath^*(\alpha_j) = \widetilde{\alpha_j}\otimes 1 \in
  H^*(Y_{m,n-1}\big((i\; i+1) \big) \times \T^{m-1}) \otimes
  H^*(D-0) \]
  And since $\delta$ is given by \fixme{Bør nok også argumenteres for
    et sted}
  \[ \delta(\beta) =
  \begin{cases}
    \widetilde{\beta}\otimes [(D,D-0)] & \text{if } \beta =
    \widetilde{\beta}\otimes [D-0] \\
    0 & \text{otherwise}
  \end{cases}, \]
  we see that $\delta(\imath^*(\alpha_j)) = 0$ in this case. If $i =
  j$, then the same considerations give us that
  \[ \delta\imath^*(\alpha_j) = 1 \otimes [(D,D-0)] \in
  H^2(Y_{m,n-1}\big((i\; i+1) \big) \times \T^{m-1}\times (D,D-0)). \]
  The group $H^2(F_1,F_0)$ are generated by exactly these elements, so
  the map $d_1$ is surjective. Since the groups involved are all free,
  we know the kernel is also free and we can calculate the rank as the
  rank of the domain minus the rank of the codomain, giving us
  \[ H^1(Y_{m,n}) \cong E_{\infty}^{0,1} = \ker d_1 \cong
  \Z^{m+n-2+m-(m-1)} = \Z^{m+n-1}. \]
\end{proof}

An immediate consequence is the following result.

\begin{corollary}
  The first cohomology group of $X_{m,n}$ is $\Z^{m-1}$.
\end{corollary}
\begin{proof}
  By using the Künneth formula and the previous theorem, we get an
  isomorphism \fixme{Henvisning til kunneth?}
  \[ H^1(X_{m,n}) \oplus H^1(\T^n) \cong  H^1(Y_{m,n}) \cong
  \Z^{m+n-1}. \]
  Since any torsion in $H^1(X_{m,n})$ would show up in $H^1(Y_{m,n})$,
  it must be free. We can calculate the rank from the above and get
  \[ H^1(X_{m,n}) \cong \Z^{m-1}. \]
\end{proof}

To make this abstract isomorphism slightly more concrete, we will
realise it as a map between spaces. Consider the image of an element
$v \in Y_{m,1}$ in $Y_{m,n}$ under the stabilisation map. This has the
form 
\[ s^{n-1}(v) = s^{n-1}
\begin{pmatrix}
  v_1 \\
  \vdots \\
  v_m
\end{pmatrix} =
\begin{pmatrix}
  \vrule & \vrule & & \vrule \\
  L^{n} v & L^{n-1}e_1 & \dots & Le_1 \\
  \vrule & \vrule & & \vrule
\end{pmatrix},
\]
where $L$ is the lower triangular matrix that defines the
stabilisation map, see Definition \ref{def:stabilisering}. Computing the
determinants gives
\[ \Det(s^{n-1}(v)) = (\pm v_1,\dots,\pm v_m, \pm 1, \dots,\pm 1), \]
with the signs dependent on $n$ and $m$. Using the action of
$\T^{n}$ to change the last $n-1$ determinants as in
\fixme{find ud af hvad der skal refereres til
  her}, we can get an element of $Y_{m,n}$ with any prescribed
determinant. Altogether this defines a lift, $f$, of the
determinant map,
\[ \xymatrix{ \T^{m+n-1} \ar[r]^f \ar@/_1.5pc/[rr]_\Id & Y_{m,n}
  \ar[r]^<<<<<\Det & \T^{m+n-1}}. \]
By taking first cohomology, we get maps
\[ \xymatrix{ H^1(Y_{m,n}) \ar@/^/[r]^{f^*} & H^1(\T^{m+n-1})
  \ar@/^/[l]^{\Det^*} }, \]
with 
\[ f^* \circ \Det^* = \Id. \]
This shows that $\Det^*$ is injective and $f^*$ is surjective. But by
the previous theorem, $f^*$ is a map between free abelian groups of
the same rank, hence it must also be injective and so an
isomorphism with inverse $\Det^*$. This shows that the determinant map
is an isomorphism on the first cohomology groups.
This result immediately extends to the quotiented spaces, with
\fixme{Sørg for at $\TT{m+n-1}{n}$ er introduceret før. Det er ikke
  den oplagte virkning...}
\[ \Det^* : H^1(\TT{m+n-1}{n}) \to H^1(X_{m,n}) \]
an isomorphism. Furthermore it shows that the stabilisation maps on
degree one cohomology are surjective, since the stabilisation maps
\[ s: Y_{m,n} \to Y_{m,n+1} \]
change the signs of some determinants and add an extra determinant
that is always plus or minus one. So if the class $d_i \in
H^1(Y_{m,n+1})$ represents the image of moving once around zero in the
$i$'th coordinate of $\T^{n+1}$, then
\[ s^*(d_i) =
\begin{cases}
  \pm d_i & 1 \leq i \leq n \\
  0 & \text{otherwise}
\end{cases} \] \fixme{Muligvis skift så vi bruger $x_i$ eller $t_i$ eller
  whatever der nu giver mening når jeg har skrevet kapitlet med
  udregninger}


\section{Quaternionic and real spaces}
\label{sec:koeff}

When the spaces $Y_{m,n}(X,Y)$ were introduced in Definition
\ref{def:rum}, we could have used other entries for our vectors than
the complex numbers. Two possible alternatives would be the real
numbers, $\R$, or the quaternions, $\Hq$, giving us the spaces
$Y^{\R}_{m,n}(X,Y)$ and $Y^{\Hq}_{m,n}(X,Y)$. These behave much like
the spaces studied above, with the notable exception that since the
quaternions are not commutative one needs to choose wheter to act with
scalars from the left or from the right. Since the stabilisation map,
\[ s : Y_{m,n}^{\Hq} \to Y_{m,n+1}^{Hq}, \]
acts on the left with a matrix and this map should be linear to give
an induced map on the quotient $X^{\Hq}_{m,n}$, we will choose to act
with scalars from the right.

      %       The theorem and the corollary illustrate the problems with using
      %       $Y_{m,n}$ instead of $X_{m,n}$, even though the space $Y_{m,n}$ is
      %       conceptually simpler. We want to consider the limit spaces
      %       $X_{m,\infty}$ and $Y_{m,\infty}$, but if we compute the first
      %       homology group of the direct limit using e.g. \cite[\S14.6]{may}, we
      %       see
      %       \begin{align*}
      %       H_1(Y_{m,\infty}) &= H_1(\varinjlim_n Y_{m,n}) = \varinjlim_n
                                  %                                   H_1(Y_{m,n}) = \varinjlim_n \Z^{m+n-1} = \Z^\infty \\
        %         H_1(X_{m,\infty}) &= H_1(\varinjlim_n X_{m,n}) = \varinjlim_n
                                      %                                       H_1(X_{m,n}) = \varinjlim_n \Z^{m-1} = \Z^{m-1}.
                                      %       \end{align*}
                                      %                                       The reason we compute homology instead of cohomology is that
                                      %                                       homology commutes with direct limits. . So for $Y_{m,\infty}$ the first homology group is not finitely generated.

                                      %%%%%                                       Previous
                                      %                                       \chapter{The space $\X$}

                                      % %                                       Snak snak. Måske deles op i to?
                                      % %                                       Noget om starten, hvorfor dette er interessant. Se bagside af blok
                                      % %                                       Også noget om rummene, definition, symmetrier (specielt X_{m,n}
                                      % %                                       \cong X_{n,m}, måske filtreringen?
                                      %                                       This chapter will introduce the spaces to be studied, along with a
                                      %                                       brief listing of various useful structure.

                                      %                                       \begin{definition}
                                      %                                       For natural numbers $m$ and $n$, the space $X_{m,n} \subset
                                      %                                       \C^{mn}$ is defined as
                                      %                                       \[ X_{m,n} = \set{(a_1,\dots,a_n) \in (\C^m)^n \delim\Bigg\vert\delim
                                      %                                       \begin{matrix}
                                      %                                       \text{Any } m \text{ subsequent vectors in } \\
        %         (e_1,\dots,e_m,a_1,\dots,a_n,e_1,\dots,e_m) \\
        %         \text{ are linearly independent.}
        %         \end{matrix} } \]
        %         For any two elements $X = [x_1,\dots,x_m]$ and $Y = [y_1,\dots,y_m]$
        %         in $\GL_m$, define the space $X_{m,n}(X,Y)$ as
        %         \[ X_{m,n}(X,Y) = \set{(a_1,\dots,a_n) \in (\C^m)^n
        %         \delim\Bigg\vert\delim
        %         \begin{matrix}
        %         \text{Any } m \text{ subsequent vectors in } \\
        %         (x_1,\dots,x_m,a_1,\dots,a_n,y_1,\dots,y_m) \\
        %         \text{ are linearly independent.}
        %         \end{matrix} } \]
        %         The special case $X=\Id$ will be denoted $\X(Y)$.
        %         Elements $(a_1,\dots,a_n)\in \X$ will be identified
        %         with the $m\times n$ matrix $A = [a_1,\dots,a_n]$ without mention.
        %         \end{definition}

        %         Since the space $\X(X,Y)$ only depends on linear independence of
        %         subsequent vectors, it can be described by giving the two flags in
        %         $\C^m$ defined from the columns of the matrices
        %         $X=\left[x_1,\dots,x_m\right]$ and $Y=\left[y_1,\dots,y_m\right]$:
        %         \begin{align*}
        %         \mathrm{Fl_R}(X) &= \Big(\spa(x_m) \subset \spa(x_{m-1},x_m) \subset
                                     %                                      \dots \subset \C^m\Big) \\
        %         \mathrm{Fl_L}(Y) &= \Big(\spa(y_1) \subset \spa(y_1,y_2) \subset
                                     %                                      \dots \subset \C^m\Big)
                                     %       \end{align*}
                                     %                                      Multiplying with $X^{-1}$ on each vector in $\X(X,Y)$ defines a
                                     %                                      homeomorphism 
                                     %                                      \[ \X(X,Y)\cong \X(\Id,X^{-1}Y) = \X(X^{-1}Y) \]
                                     %                                      Hence we only need to consider these spaces.

                                     %                                      Note that we can also define the space $X_{m,n}$ as the preimage of
                                     %                                      $(\C^*)^{m+n-1}$ under the map $\Det : (\C^m)^n \to \C^{m+n-1}$,
                                     %                                      given by
                                     %                                      \begin{align*}
                                     %                                      \Det(a_1,\dots,a_n) = \Big( &\det(e_2,\dots,e_m,a_1),\\
        %                      &\det(e_3,\dots,e_m,a_1,a_2),\\
        %                      &\dots, \\
        %                      &\det(a_n,e_1,\dots,e_{m-1}) \Big) 
                                 %       \end{align*}
                                 %                                  where $\det$ is the determinant map. There is a similar description
                                 %                                  of $X_{m,n}(Y)$ as the preimage under a map $\Det_{Y}$. Since the map
                                 %                                  $\Det_Y$ is continuous, this shows that the space $\X(Y)$ is open in
                                 %                                  $\C^{mn}$.

                                 %                                  \section{Stabilization}

                                 %                                  We would like to relate the spaces $X_{m,n}$ and $X_{m,n+1}$. To do
                                 %                                  this, consider a lower-triangular $m\times m$ invertible matrix
                                 %                                  $L$. This preserves the right flag $\mathrm{Fl_R}(\Id)$, so it defines
                                 %                                  a homeomorphism $\tilde L : \X(Y) \to \X(LY)$ by
                                 %                                  \begin{align*}
                                 %                                  \tilde L(a_1,\dots,a_n) &= (La_1,\dots,La_n)
                                                                                              %       \end{align*}

                                                                                              %                                                                                               By using the Bruhat decompositon of the general linear group,
                                                                                              %                                                                                               any matrix $Y$ can be written uniquely as a product $L \sigma U$,
                                                                                              %                                                                                               where $L$ is an invertible lower triangular matrix, $U$ is an
                                                                                              %                                                                                               invertible upper triangular matrix, and $\sigma$ is a
                                                                                              %                                                                                               permutation matrix, see \cite[Example~1.2.11]{bjorner} or
                                                                                              %                                                                                               \cite[Proposition~4.5]{hiller}. Since the flag $\mathrm{Fl_L}(\sigma
                                                                                              %                                                                                               U)$ is preserved by right multiplication with upper triangular
                                                                                              %                                                                                               matrices, this only depends on the permutation $\sigma$ and not on
                                                                                              %                                                                                               $U$. All of this together shows the following lemma:

                                                                                              %                                                                                               \begin{lemma}
                                                                                              %                                                                                               For $Y\in \GL_m$ with $Y = L\sigma U$, the space $\X(Y)$ is
                                                                                              %                                                                                               homeomorphic to $\X(\sigma)$.
                                                                                              %                                                                                               \end{lemma}

                                                                                              %                                                                                               Using this lemma, we only need to understand the spaces
                                                                                              %                                                                                               $\X(\sigma)$ to understand all of the spaces $\X(Y)$. More importantly,
                                                                                              %                                                                                               it turns out that the space $X_{m,n+1}(\sigma)$ is given as a union of
                                                                                              %                                                                                               spaces homeomorphic to a torus times $\X(\tau)$, for various
                                                                                              %                                                                                               permutations $\tau$. This is summarized in the following lemma.

                                                                                              %                                                                                               \begin{lemma}
                                                                                              %                                                                                               Any choice of indices $I= (i_1<\dots<i_k) \subset \set{1,\dots,m}$
                                                                                              %                                                                                               gives a subspace of $X_{m,n+1}(\sigma)$:
                                                                                              %                                                                                               \[ X_{m,n+1}^I(\sigma) = \set{(a_1,\dots,a_{n+1}) \in
                                                                                              %                                                                                               X_{m,n+1}(\sigma) \sdel 
                                                                                              %                                                                                               \begin{matrix} 
                                                                                              %                                                                                               (a_{n+1})_{i_j} \neq 0\; \forall i_j \in I, \\
        %         (a_{n+1})_j = 0 \;\forall j \not\in I
        %         \end{matrix} } \]
        %         This space is empty if $I$ does not contain the number $\sigma(m)$,
        %         so we will assume that $\sigma(m) \in I$ but otherwise leave it out
        %         of the notation.

        %         These subspaces cover $X_{m,n+1}(\sigma)$. There is a map $\varphi$,
        %         defined for an indexing set $I$ and a permutation $\sigma$, which
        %         gives a new permutation such that
        %         \[ X_{m,n+1}^I(\sigma) \cong \X(\varphi(I,\sigma)) \times
        %         (\C^*)^{\size{I}} \]
        %         If we define the permutation $\widehat \sigma = \sigma \cdot
        %         \big(m\; m-1\; \dots \; 1\big)$ and the indexing set is $I = (i_1 <
        %         \dots < i_k)$, then $\varphi$ is given by
        %         \[ \varphi\big(I,\sigma\big) = \left(\prod_{j \in J} \big(
        %         i_j \; \sigma(m) \big) \right) \widehat \sigma \]
        %         where the product is over the set
        %         \[ J = \set{ j \in\set{1,\dots,k} \delim\mid\delim i_j <
        %         \sigma(m),\; \widehat\sigma^{-1}(i_j) > \widehat\sigma^{-1}(i_r) \,
        %         \forall r < j } \]
        %         \end{lemma}

        %         The proof of this lemma relies on taking $X_{m,n+1}^I(\sigma)$ and
        %         multiplying with a lower triangular matrix to reduce it to something
        %         in $\X(\tau)$. By considering the matrix used, the formula
        %         for $\tau = \varphi(I,\sigma)$ given in the lemma can be found. In
        %         practice, it is
        %         often easier to do the reduction by hand rather than using the lemma,
        %         but the formula can be useful. For example, it is clear that
        %         $\varphi(I,\sigma)$ is given as a product of transpositions
        %         and $\varphi\big((\sigma(m)), \sigma\big)$. By
        %         considering exactly which transpositions, we can show that for all $I$
        %         we have
        %         \[ \varphi(I,\sigma) \leq \varphi\big((\sigma(m)), \sigma\big) \] 
        %         in the Bruhat ordering on the symmetric group. The Bruhat
        %         order is defined in \cite{bjorner}. It also follows that for all
        %         $\sigma$, 
        %         \[ \varphi\big((1<2<\dots<m),\sigma\big) = \Id \]
        %         With this we can identify $X_{m,n}$ with the subspace
        %         of $X_{m,n+1}$ where the last column has a 1 on each entry:
        %         \[ X_{m,n} \cong \set{ (a_1,\dots,a_{n+1}) \in X_{m,n+1} \mid
        %         a_{n+1} = (1,\dots,1) } \]
        %         The identification is given by the map $s : \X \to X_{m,n+1}$,
        %         \[ s(a_1,\dots,a_n) = (La_1,\dots,La_n,Le_1) \]
        %         where $L$ is the lower triangular matrix with all entries on or below
        %         the diagonal equal to one:
        %         \[ L =
        %         \begin{pmatrix}
        %         1 & 0 & \dots & 0 \\
        %         \vdots & \ddots & \ddots & \vdots \\
        %         \vdots &  & \ddots & 0 \\
        %         1 & \dots & \dots & 1
                                      %                                       \end{pmatrix} \]

                                      %                                       These maps give a directed system of spaces,
                                      %                                       \[ \xymatrix{ X_{m,1} \ar[r]^s & X_{m,2} \ar[r]^s & \dots \ar[r]^s &
                                                                                                                                                   %                                                                                                                                                    X_{m,n} \ar[r]^s & X_{m,n+1} \ar[r]^s & \dots } \]
                                                                                                                                                                                                                                                                                                                                                %                                                                                                                                                                                                                                                                                                                                                 and we can take the direct limit of this system,
                                                                                                                                                                                                                                                                                                                                                %                                                                                                                                                                                                                                                                                                                                                 \[ X_{m,\infty} = \varinjlim_n \X \]
                                                                                                                                                                                                                                                                                                                                                %                                                                                                                                                                                                                                                                                                                                                 The direct limit is the disjoint union of all the spaces $\X$, with the
                                                                                                                                                                                                                                                                                                                                                %                                                                                                                                                                                                                                                                                                                                                 identification $A \sim B$ if there are $j,k$ such that
                                                                                                                                                                                                                                                                                                                                                %                                                                                                                                                                                                                                                                                                                                                 $s^k(A) = s^j(B)$. The topology is the finest such that the maps $\X
                                                                                                                                                                                                                                                                                                                                                %                                                                                                                                                                                                                                                                                                                                                 \to X_{m,\infty}$ are all continuous.

                                                                                                                                                                                                                                                                                                                                                %                                                                                                                                                                                                                                                                                                                                                 \section{Symmetries}

                                                                                                                                                                                                                                                                                                                                                % %                                                                                                                                                                                                                                                                                                                                                 Skriv noget her og byt rundt på rækkefølge?
                                                                                                                                                                                                                                                                                                                                                % %                                                                                                                                                                                                                                                                                                                                                 Noget om virkninger af \C^* på \X, e.g. virk på søjler, rækker,...
                                                                                                                                                                                                                                                                                                                                                %                                                                                                                                                                                                                                                                                                                                                 There are various symmetries and group actions worth keeping in mind
                                                                                                                                                                                                                                                                                                                                                %                                                                                                                                                                                                                                                                                                                                                 when working with these spaces. Some of these will be described here.

                                                                                                                                                                                                                                                                                                                                                %                                                                                                                                                                                                                                                                                                                                                 Since we are interested in linear independence of vectors in $\C^m$,
                                                                                                                                                                                                                                                                                                                                                %                                                                                                                                                                                                                                                                                                                                                 the group of complex units $\C^*$ acts on $\X(\sigma)$ in various
                                                                                                                                                                                                                                                                                                                                                %                                                                                                                                                                                                                                                                                                                                                 ways. For any index $i$, we can define a group action $l_i$ by scaling
                                                                                                                                                                                                                                                                                                                                                %                                                                                                                                                                                                                                                                                                                                                 the $i$\textsuperscript{th} vector:
                                                                                                                                                                                                                                                                                                                                                %                                                                                                                                                                                                                                                                                                                                                 \begin{align*}
                                                                                                                                                                                                                                                                                                                                                %                                                                                                                                                                                                                                                                                                                                                 l_i : \C^* \times \X(\sigma) &\to \X(\sigma) \\
        %         \big(\lambda , (a_1,\dots,a_n)\big) &\mapsto
                                                        %                                                         \big(a_1,\dots,a_{i-1},\lambda a_i,a_{i+1},\dots,a_n\big) 
                                                        %       \end{align*}
                                                        %                                                         These actions commute with each other, so we get an action $l$ of
                                                        %                                                         $(\C^*)^n$ on $\X(\sigma)$, given by
                                                        %                                                         \[ l\big((\l_1,\dots,\l_n),(a_1,\dots,a_n)\big) =
                                                        %                                                         \big(\l_1a_1,\dots,\l_na_n\big) \]

                                                        %                                                         Likewise, we can also multiply the rows of $A\in
                                                        %                                                         \X(\sigma)$ by non-zero complex numbers. This defines an action $l'$
                                                        %                                                         of $(\C^*)^m$ on $\X(\sigma)$. The two actions together satisfy the
                                                        %                                                         relation 
                                                        %                                                         \[ l\Big((\l,\dots,\l),l'\big((\l^{-1},\dots,\l^{-1}), A\big)\Big) =
  % A \]
  % since multiplying all rows by $\l^{-1}$ while multiplying all columns
  % by $\l$ is the same as doing nothing.

  % For the special case of $\X = \X(\Id)$, there is an additional
  % symmetry that can be useful to know. By thinking of $A \in \X$ as a
  % matrix, it is possible to transpose it. Checking the various
  % determinants used in the definition of $\X$ show that they do not
  % change after transposition. Hence transposing defines a homeomorphism
  % $T : X_{m,n} \to X_{n,m}$, allowing us to switch the order of $m$
  % and $n$.

  % \section{The quotient space}

  % % Rummene Y_{m,n}. 
  % While the above definition of $\X$ gives an open subset of $\C^{mn}$,
  % it has some issues that makes the space hard to
  % work with. As we shall see shortly, the fundamental group and the
  % cohomology ring of $\X$ both become larger as $n$ grows.
  % This complicates things slightly, but it can be fixed by taking the
  % quotient of a group action.
  % \begin{definition}
  %   The space $Y_{m,n}$ is the quotient space
  %   \[ Y_{m,n} = \X / T^n \]
  %   where $T^n$ acts on $X_{m,n}$ by scaling
  %   the columns, as defined above:
  %   \[ (\lambda_1,\dots,\lambda_n)\cdot (a_1,\dots,a_n) =
  %   (\l_1a_1,\dots,\l_na_n) \]
  % \end{definition}

  % These spaces retain some of the structure described above. For
  % example, the stabilization map $s$ is linear in $A$, which means it
  % respects the group action.
  % \[ s\big((\l_1,\dots,\l_n) \cdot A\big) = (\l_1,\dots,\l_n,1)\cdot
  % s(A) \] 
  % Hence $s$ descends to a stabilization map $s : Y_{m,n} \to
  % Y_{m,n+1}$ and there is a direct limit,
  % \[ Y_{m,\infty} = \varinjlim_{n} Y_{m,n} \]
  % which can be thought of as $X_{m,\infty}/T^\infty$.

  % The space $Y_{m,n}$ is homeomorphic to the subspace of $\X$ consisting
  % of elements with the last $n$ coordinates of the determinant map
  % $\Det$ equal to 1:
  % \[ Y_{m,n} \cong \set{ A \in \X \delim\big\vert\delim \Det(A) \in
  % (\C^*)^{m-1}\times \set{1}^n }\]
  % The group action gives an identification,
  % \[ \X \cong Y_{m,n}\times (\C^*)^n \]
  % This shows the claim about the fundamental group of $\X$, but it can
  % also be used to get information about $Y_{m,n}$ from $\X$ or
  % vice versa. For example, it allows us to calculate the cohomology of
  % one when we know the cohomology of the other, by using the
  % K\"unneth formula to add or remove the cohomology of $(\C^*)^n$.



  %%% Local Variables: 
  %%% mode: latex
  %%% TeX-master: "main"
  %%% End: 
